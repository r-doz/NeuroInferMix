{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17d96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from representation.bayesian_nn import BayesLinearGMM, BNN_GMM\n",
    "from representation.gaussian_mixture import GaussianMix\n",
    "\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1562ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== From-scratch init (default reset_parameters) ===\n",
      "Layer 0 pi_w shape: torch.Size([5, 3, 4])\n",
      "Layer 0 mu_w mean/std: 0.000469229620648548 0.3450576961040497\n",
      "Layer 0 sigma_w min: 0.028867512941360474\n",
      "Layer 0 pi_w row-sum (should be ~1): 1.0\n",
      "\n",
      "=== Init-from-deterministic ===\n",
      "Max |mu_w[...,0] - W_det|: 0.0\n",
      "Max |mu_b[:,0]  - b_det|: 0.0\n",
      "Layer 0 pi_w[0,0,:]: tensor([1.0000e+00, 1.0000e-12, 1.0000e-12, 1.0000e-12])\n",
      "Layer 0 sigma_w[0,0,:]: tensor([0.0010, 0.0010, 0.0010, 0.0010])\n"
     ]
    }
   ],
   "source": [
    "# 1) Create a toy BNN: input=3, hidden=5, output=2, K=4 mixture components\n",
    "bnn = BNN_GMM(layer_sizes=[3, 5, 2], K=4, bias=True).to(device)\n",
    "\n",
    "print(\"=== From-scratch init (default reset_parameters) ===\")\n",
    "print(\"Layer 0 pi_w shape:\", bnn.layers[0].pi_w.shape)   # (5, 3, 4)\n",
    "print(\"Layer 0 mu_w mean/std:\", bnn.layers[0].mu_w.mean().item(), bnn.layers[0].mu_w.std().item())\n",
    "print(\"Layer 0 sigma_w min:\", bnn.layers[0].sigma_w.min().item())\n",
    "print(\"Layer 0 pi_w row-sum (should be ~1):\",\n",
    "        bnn.layers[0].pi_w[0, 0, :].sum().item())\n",
    "\n",
    "# 2) Create a deterministic MLP with matching shapes\n",
    "det = nn.Sequential(\n",
    "    nn.Linear(3, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2),\n",
    ").to(device)\n",
    "\n",
    "# (Optional) pretend it's \"trained\" by just doing one fake optimizer step\n",
    "opt = torch.optim.SGD(det.parameters(), lr=0.1)\n",
    "x = torch.randn(16, 3, device=device)\n",
    "y = torch.randn(16, 2, device=device)\n",
    "loss = ((det(x) - y) ** 2).mean()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "# 3) Initialize BNN mixture params from deterministic net (component 0 matches det weights)\n",
    "bnn.init_from_deterministic_mlp(det, sigma0=1e-3, main_comp=0)\n",
    "\n",
    "print(\"\\n=== Init-from-deterministic ===\")\n",
    "# Check that component 0 matches\n",
    "diff_W0 = (bnn.layers[0].mu_w[..., 0] - det[0].weight.data).abs().max().item()\n",
    "diff_b0 = (bnn.layers[0].mu_b[:, 0] - det[0].bias.data).abs().max().item()\n",
    "print(\"Max |mu_w[...,0] - W_det|:\", diff_W0)\n",
    "print(\"Max |mu_b[:,0]  - b_det|:\", diff_b0)\n",
    "print(\"Layer 0 pi_w[0,0,:]:\", bnn.layers[0].pi_w[0, 0, :].detach().cpu())\n",
    "print(\"Layer 0 sigma_w[0,0,:]:\", bnn.layers[0].sigma_w[0, 0, :].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99296ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = BNN_GMM(layer_sizes=[2, 3, 1], K=4, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55942ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       "\n",
       "        [[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       "\n",
       "        [[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn.layers[0].pi_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f868ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neuron_preactivation(input : GaussianMix, weights : torch.Tensor, bias=None):\n",
    "    \"\"\"\n",
    "    Compute the pre-activation Gaussian mixture of a neuron given input Gaussian mixture and weights.\n",
    "\n",
    "    Args:\n",
    "        input (GaussianMix): Input Gaussian mixture with shape (batch_size, input_dim, K).\n",
    "        weights (torch.Tensor): Weights of the neuron, each of them is a gaussian mixture and thus have pi, mu, sigma.\n",
    "        bias (torch.Tensor, optional): Bias of the neuron, it is also a gaussian mixture. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return \n",
    "\n",
    "def gm_product(pi_w, mu_w, sigma_w, pi_x, mu_x, sigma_x, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Product of two independent scalar Gaussian mixtures (moment-matched).\n",
    "\n",
    "    Inputs:\n",
    "      pi_w, mu_w, sigma_w: (Kw,)\n",
    "      pi_x, mu_x, sigma_x: (Kx,)\n",
    "\n",
    "    Returns:\n",
    "      pi_y:     (Kw*Kx,)\n",
    "      mu_y:     (Kw*Kx,)\n",
    "      sigma_y:  (Kw*Kx,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure 1D\n",
    "    pi_w, mu_w, sigma_w = pi_w.view(-1), mu_w.view(-1), sigma_w.view(-1)\n",
    "    pi_x, mu_x, sigma_x = pi_x.view(-1), mu_x.view(-1), sigma_x.view(-1)\n",
    "\n",
    "    # ---- Cartesian product via broadcasting ----\n",
    "    # Weights\n",
    "    pi_y = (pi_w[:, None] * pi_x[None, :]).reshape(-1)\n",
    "\n",
    "    # Means\n",
    "    mu_y = (mu_w[:, None] * mu_x[None, :]).reshape(-1)\n",
    "\n",
    "    # Variances\n",
    "    var_y = (\n",
    "        (sigma_w[:, None] ** 2) * (sigma_x[None, :] ** 2)\n",
    "        + (sigma_w[:, None] ** 2) * (mu_x[None, :] ** 2)\n",
    "        + (sigma_x[None, :] ** 2) * (mu_w[:, None] ** 2)\n",
    "    ).reshape(-1)\n",
    "\n",
    "    sigma_y = torch.sqrt(var_y.clamp_min(eps))\n",
    "\n",
    "    # Normalize weights (important numerically)\n",
    "    pi_y = pi_y.clamp_min(eps)\n",
    "    pi_y = pi_y / pi_y.sum()\n",
    "\n",
    "    return pi_y, mu_y, sigma_y\n",
    "\n",
    "\n",
    "def gm_times_scalar(pi, mu, sigma, x):\n",
    "    \"\"\"\n",
    "    X deterministic scalar, W ~ Gaussian mixture\n",
    "    \"\"\"\n",
    "    pi_y = pi\n",
    "    mu_y = mu * x\n",
    "    sigma_y = sigma * x.abs()\n",
    "    return pi_y, mu_y, sigma_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6afbb13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Product GM sanity check (moment test) ===\n",
      "Output components: 4 (should be Kw*Kx = 4)\n",
      "pi_y sums to: 1.000000000000\n",
      "\n",
      "Predicted mean: -0.770000   | MC mean: -0.765026   | abs diff: 4.973549e-03\n",
      "Predicted var :  4.474246   | MC var :  4.459763   | abs diff: 1.448291e-02\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Utilities: mixture moments\n",
    "# ----------------------------\n",
    "def mix_mean_var(pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Moments of a scalar Gaussian mixture.\n",
    "    pi, mu, sigma: (K,)\n",
    "    Returns: mean, var\n",
    "    \"\"\"\n",
    "    mean = torch.sum(pi * mu)\n",
    "    second_moment = torch.sum(pi * (sigma**2 + mu**2))\n",
    "    var = second_moment - mean**2\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Monte Carlo sampling of a mixture\n",
    "# ----------------------------\n",
    "def sample_gm(pi, mu, sigma, N: int):\n",
    "    \"\"\"\n",
    "    Sample N points from a scalar Gaussian mixture.\n",
    "    pi, mu, sigma: (K,)\n",
    "    returns: (N,)\n",
    "    \"\"\"\n",
    "    K = pi.numel()\n",
    "    # Sample component indices\n",
    "    idx = torch.multinomial(pi, num_samples=N, replacement=True)  # (N,)\n",
    "    # Sample from the chosen normal component\n",
    "    eps = torch.randn(N, device=pi.device, dtype=pi.dtype)\n",
    "    return mu[idx] + sigma[idx] * eps\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Test / demo\n",
    "# ----------------------------\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define two mixtures with small K (easy to inspect)\n",
    "pi_w = torch.tensor([0.3, 0.7], dtype=torch.float64)\n",
    "mu_w = torch.tensor([-1.0,  2.0], dtype=torch.float64)\n",
    "sg_w = torch.tensor([ 0.5,  0.2], dtype=torch.float64)\n",
    "\n",
    "pi_x = torch.tensor([0.4, 0.6], dtype=torch.float64)\n",
    "mu_x = torch.tensor([ 0.5, -1.5], dtype=torch.float64)\n",
    "sg_x = torch.tensor([ 0.3,  0.4], dtype=torch.float64)\n",
    "\n",
    "# (Optional sanity checks)\n",
    "assert torch.isclose(pi_w.sum(), torch.tensor(1.0, dtype=torch.float64))\n",
    "assert torch.isclose(pi_x.sum(), torch.tensor(1.0, dtype=torch.float64))\n",
    "assert (sg_w > 0).all() and (sg_x > 0).all()\n",
    "\n",
    "# Compute product-mixture (approx)\n",
    "pi_y, mu_y, sg_y = gm_product(pi_w, mu_w, sg_w, pi_x, mu_x, sg_x)\n",
    "\n",
    "# Predicted moments from the output mixture\n",
    "m_pred, v_pred = mix_mean_var(pi_y, mu_y, sg_y)\n",
    "\n",
    "# Monte Carlo check\n",
    "N = 500_000\n",
    "w_s = sample_gm(pi_w, mu_w, sg_w, N)\n",
    "x_s = sample_gm(pi_x, mu_x, sg_x, N)\n",
    "y_s = w_s * x_s\n",
    "\n",
    "m_mc = y_s.mean()\n",
    "v_mc = y_s.var(unbiased=False)\n",
    "\n",
    "print(\"=== Product GM sanity check (moment test) ===\")\n",
    "print(f\"Output components: {pi_y.numel()} (should be Kw*Kx = {pi_w.numel()*pi_x.numel()})\")\n",
    "print(f\"pi_y sums to: {pi_y.sum().item():.12f}\")\n",
    "print()\n",
    "print(f\"Predicted mean: {m_pred.item(): .6f}   | MC mean: {m_mc.item(): .6f}   | abs diff: {abs(m_pred-m_mc).item():.6e}\")\n",
    "print(f\"Predicted var : {v_pred.item(): .6f}   | MC var : {v_mc.item(): .6f}   | abs diff: {abs(v_pred-v_mc).item():.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b77237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gm_product_per_neuron(pi_w_j, mu_w_j, sigma_w_j, pi_x, mu_x, sigma_x, eps=1e-12, normalize_pi=True):\n",
    "\n",
    "    I, Kw = pi_w_j.shape\n",
    "    I2, Kx = pi_x.shape\n",
    "    assert I == I2\n",
    "\n",
    "    # Broadcast to (I, Kw, Kx)\n",
    "    pi = pi_w_j.unsqueeze(-1) * pi_x.unsqueeze(-2)\n",
    "    mu = mu_w_j.unsqueeze(-1) * mu_x.unsqueeze(-2)\n",
    "\n",
    "    var = (sigma_w_j.unsqueeze(-1)**2) * (sigma_x.unsqueeze(-2)**2) \\\n",
    "        + (sigma_w_j.unsqueeze(-1)**2) * (mu_x.unsqueeze(-2)**2) \\\n",
    "        + (sigma_x.unsqueeze(-2)**2) * (mu_w_j.unsqueeze(-1)**2)\n",
    "\n",
    "    sigma = torch.sqrt(var.clamp_min(eps))\n",
    "\n",
    "    # Flatten (Kw, Kx) -> (Kw*Kx)\n",
    "    pi = pi.reshape(I, Kw * Kx)\n",
    "    mu = mu.reshape(I, Kw * Kx)\n",
    "    sigma = sigma.reshape(I, Kw * Kx)\n",
    "\n",
    "    #if normalize_pi:\n",
    "    #    pi = pi.clamp_min(eps)\n",
    "    #    pi = pi / pi.sum(dim=-1, keepdim=True).clamp_min(eps)\n",
    "    \n",
    "    # put to 0 weights smaller than eps, then renormalize\n",
    "    mask = pi > eps\n",
    "    pi = pi * mask\n",
    "    pi = pi / pi.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return pi, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e458548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.4496],\n",
      "        [-0.4856],\n",
      "        [ 0.0758]], grad_fn=<SelectBackward0>)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[-0.4496],\n",
      "        [-0.2428],\n",
      "        [-0.0227]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0.0946],\n",
      "        [0.0507],\n",
      "        [0.0258]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "I  = 3\n",
    "Kx = 1\n",
    "\n",
    "pi_x = torch.tensor([\n",
    "    [1],   # x_0\n",
    "    [1],   # x_1\n",
    "    [1],   # x_2\n",
    "])                      # shape: (I, Kx)\n",
    "\n",
    "mu_x = torch.tensor([\n",
    "    [ 1.0],\n",
    "    [ 0.5],\n",
    "    [-0.3],\n",
    "])                      # shape: (I, Kx)\n",
    "\n",
    "sigma_x = torch.tensor([\n",
    "    [0.2],\n",
    "    [0.1],\n",
    "    [0.3],\n",
    "])                      # shape: (I, Kx)\n",
    "\n",
    "bnn = BNN_GMM(layer_sizes=[3, 2, 1], K=1, bias=True).to(device)\n",
    "\n",
    "layer = bnn.layers[0]\n",
    "j = 0  # neuron index\n",
    "\n",
    "pi_y, mu_y, sigma_y = gm_product_per_neuron(\n",
    "    layer.pi_w[j], layer.mu_w[j], layer.sigma_w[j],   # (I, Kw)\n",
    "    pi_x, mu_x, sigma_x)\n",
    "\n",
    "print(layer.pi_w[j])\n",
    "print(layer.mu_w[j])\n",
    "print(pi_y)\n",
    "print(mu_y)\n",
    "print(sigma_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e327f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.],\n",
       "        [1.]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn.layers[0].pi_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84d3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2498], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gm_product_per_neuron(pi_w_j, mu_w_j, sigma_w_j, pi_x, mu_x, sigma_x, eps=1e-12, normalize_pi=True):\n",
    "\n",
    "    I, Kw = pi_w_j.shape\n",
    "    I2, Kx = pi_x.shape\n",
    "    assert I == I2\n",
    "\n",
    "    # Broadcast to (I, Kw, Kx)\n",
    "    pi = pi_w_j.unsqueeze(-1) * pi_x.unsqueeze(-2)\n",
    "    mu = mu_w_j.unsqueeze(-1) * mu_x.unsqueeze(-2)\n",
    "\n",
    "    var = (sigma_w_j.unsqueeze(-1)**2) * (sigma_x.unsqueeze(-2)**2) \\\n",
    "        + (sigma_w_j.unsqueeze(-1)**2) * (mu_x.unsqueeze(-2)**2) \\\n",
    "        + (sigma_x.unsqueeze(-2)**2) * (mu_w_j.unsqueeze(-1)**2)\n",
    "\n",
    "    sigma = torch.sqrt(var.clamp_min(eps))\n",
    "\n",
    "    # Flatten (Kw, Kx) -> (Kw*Kx)\n",
    "    pi = pi.reshape(I, Kw * Kx)\n",
    "    mu = mu.reshape(I, Kw * Kx)\n",
    "    sigma = sigma.reshape(I, Kw * Kx)\n",
    "    \n",
    "    # put to 0 weights smaller than eps, then renormalize\n",
    "    mask = pi > eps\n",
    "    pi = pi * mask\n",
    "    pi = pi / pi.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return pi, mu, sigma\n",
    "\n",
    "def gm_add(pi1, mu1, sg1, pi2, mu2, sg2, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Add two independent scalar GMs:\n",
    "      (pi1, mu1, sg1): (K1,)\n",
    "      (pi2, mu2, sg2): (K2,)\n",
    "    Returns (pi, mu, sg): (<=max_components,)\n",
    "    \"\"\"\n",
    "    # Cartesian product\n",
    "    w = (pi1[:, None] * pi2[None, :]).reshape(-1)\n",
    "    mu = (mu1[:, None] + mu2[None, :]).reshape(-1)\n",
    "    var = (sg1[:, None]**2 + sg2[None, :]**2).reshape(-1)\n",
    "    sg = torch.sqrt(var.clamp_min(eps))\n",
    "   \n",
    "    # put to 0 weights smaller than eps, then renormalize\n",
    "    mask = w > eps\n",
    "    w = w * mask\n",
    "    w = w / w.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return w, mu, sg\n",
    "\n",
    "\n",
    "def sum_over_inputs_tree(mixtures, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Tree-reduce sum of I independent GMs (per-neuron).\n",
    "    Inputs:\n",
    "      pi_y, mu_y, sg_y: (I, K)  (K can be Kw*Kx from your product)\n",
    "    Output:\n",
    "      pi_z, mu_z, sg_z: (<=max_components,)\n",
    "    \"\"\"\n",
    "    level = mixtures\n",
    "    while len(level) > 1:\n",
    "        new = []\n",
    "        for i in range(0, len(level), 2):\n",
    "            if i + 1 < len(level):\n",
    "                pi1, mu1, sg1 = level[i]\n",
    "                pi2, mu2, sg2 = level[i+1]\n",
    "                new.append(gm_add(pi1, mu1, sg1, pi2, mu2, sg2,\n",
    "                                            eps=eps))\n",
    "            else:\n",
    "                new.append(level[i])\n",
    "        level = new\n",
    "    return level[0]\n",
    "\n",
    "\n",
    "def preactivation_gm_for_neuron_tree(layer, j, pi_x, mu_x, sg_x, eps=1e-12):\n",
    "    \"\"\"\n",
    "    z_j = sum_i w_{j,i} x_i + b_j\n",
    "    \"\"\"\n",
    "    # product per input i => (I, Kprod)\n",
    "    pi, mu, sigma = gm_product_per_neuron(\n",
    "        layer.pi_w[j], layer.mu_w[j], layer.sigma_w[j],\n",
    "        pi_x, mu_x, sg_x,\n",
    "        eps=eps, normalize_pi=True\n",
    "    )\n",
    "    mixtures = [(pi[i], mu[i], sigma[i]) for i in range(pi.shape[0])]\n",
    "\n",
    "    # tree sum over i\n",
    "    pi_z, mu_z, sg_z = sum_over_inputs_tree(mixtures, eps=eps)\n",
    "\n",
    "    # add bias mixture\n",
    "    if layer.bias:\n",
    "        pi_z, mu_z, sg_z = gm_add(\n",
    "            pi_z, mu_z, sg_z,\n",
    "            layer.pi_b[j], layer.mu_b[j], layer.sigma_b[j], eps=eps\n",
    "        )\n",
    "\n",
    "    return pi_z, mu_z, sg_z\n",
    "\n",
    "bnn = BNN_GMM(layer_sizes=[3, 2, 1], K=1, bias=True).to(device)\n",
    "layer = bnn.layers[0]\n",
    "pi_z, mu_z, sg_z = preactivation_gm_for_neuron_tree(layer, j=0, pi_x=pi_x, mu_x=mu_x, sg_x=sigma_x)\n",
    "\n",
    "mu_z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981d8b6",
   "metadata": {},
   "source": [
    "NOW let's handle batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_renorm(w, dim=-1, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Renormalize along `dim`, safely avoiding division by zero.\n",
    "    If a row sums to 0 after thresholding, it leaves it as all-zeros.\n",
    "    \"\"\"\n",
    "    s = w.sum(dim=dim, keepdim=True)\n",
    "    return torch.where(s > 0, w / s.clamp_min(eps), w)\n",
    "\n",
    "\n",
    "def gm_product_per_neuron(pi_w_j, mu_w_j, sigma_w_j,\n",
    "                          pi_x, mu_x, sigma_x,\n",
    "                          eps=1e-12, normalize_pi=True):\n",
    "    \"\"\"\n",
    "    Product per input i (per-neuron), batched over B.\n",
    "\n",
    "    Weights:\n",
    "      pi_w_j, mu_w_j, sigma_w_j: (I, Kw)\n",
    "\n",
    "    Inputs x:\n",
    "      pi_x, mu_x, sigma_x: either (I, Kx) or (B, I, Kx)\n",
    "\n",
    "    Returns:\n",
    "      pi, mu, sigma: (B, I, Kw*Kx)  (or (I, Kw*Kx) if B not provided)\n",
    "    \"\"\"\n",
    "    I, Kw = pi_w_j.shape\n",
    "\n",
    "    # Make x batched: (B, I, Kx)\n",
    "    if pi_x.dim() == 2:\n",
    "        # (I, Kx) -> (1, I, Kx) (broadcastable)\n",
    "        pi_x = pi_x.unsqueeze(0)\n",
    "        mu_x = mu_x.unsqueeze(0)\n",
    "        sigma_x = sigma_x.unsqueeze(0)\n",
    "\n",
    "    B, I2, Kx = pi_x.shape\n",
    "    assert I == I2, f\"I mismatch: weights have {I}, inputs have {I2}\"\n",
    "\n",
    "    # Broadcast to (B, I, Kw, Kx)\n",
    "    pi = pi_w_j.unsqueeze(0).unsqueeze(-1) * pi_x.unsqueeze(-2)\n",
    "    mu = mu_w_j.unsqueeze(0).unsqueeze(-1) * mu_x.unsqueeze(-2)\n",
    "\n",
    "    # Var for product of independent Gaussians (your formula)\n",
    "    sw2 = (sigma_w_j.unsqueeze(0).unsqueeze(-1) ** 2)  # (B, I, Kw, 1)\n",
    "    sx2 = (sigma_x.unsqueeze(-2) ** 2)                 # (B, I, 1, Kx)\n",
    "    mw2 = (mu_w_j.unsqueeze(0).unsqueeze(-1) ** 2)     # (B, I, Kw, 1)\n",
    "    mx2 = (mu_x.unsqueeze(-2) ** 2)                    # (B, I, 1, Kx)\n",
    "\n",
    "    var = sw2 * sx2 + sw2 * mx2 + sx2 * mw2\n",
    "    sigma = torch.sqrt(var.clamp_min(eps))\n",
    "\n",
    "    # Flatten (Kw, Kx) -> (Kw*Kx): (B, I, Kprod)\n",
    "    Kprod = Kw * Kx\n",
    "    pi = pi.reshape(B, I, Kprod)\n",
    "    mu = mu.reshape(B, I, Kprod)\n",
    "    sigma = sigma.reshape(B, I, Kprod)\n",
    "\n",
    "    if normalize_pi:\n",
    "        # Threshold tiny weights, then renormalize per (B, I, :)\n",
    "        pi = pi * (pi > eps)\n",
    "        pi = _safe_renorm(pi, dim=-1, eps=eps)\n",
    "\n",
    "    # If original x was unbatched, return unbatched to preserve old behavior\n",
    "    if B == 1 and pi_w_j.dim() == 2 and (pi_x is not None) and (pi_x.shape[0] == 1):\n",
    "        # NOTE: This keeps it compatible with your old code path;\n",
    "        # if you prefer always-batched outputs, just delete this block.\n",
    "        return pi.squeeze(0), mu.squeeze(0), sigma.squeeze(0)\n",
    "\n",
    "    return pi, mu, sigma\n",
    "\n",
    "\n",
    "def gm_add(pi1, mu1, sg1, pi2, mu2, sg2, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Add two independent scalar GMs, batch-aware.\n",
    "\n",
    "    Supports:\n",
    "      - (K1,) + (K2,) -> (K1*K2,)\n",
    "      - (B,K1) + (B,K2) -> (B,K1*K2)\n",
    "      - (B,K1) + (K2,)  -> bias broadcast across batch\n",
    "      - (K1,)  + (B,K2) -> broadcast across batch\n",
    "    \"\"\"\n",
    "    # Remember original batched-ness\n",
    "    batched1 = (pi1.dim() == 2)\n",
    "    batched2 = (pi2.dim() == 2)\n",
    "    batched = batched1 or batched2\n",
    "\n",
    "    # Promote to batched\n",
    "    if pi1.dim() == 1:\n",
    "        pi1, mu1, sg1 = pi1.unsqueeze(0), mu1.unsqueeze(0), sg1.unsqueeze(0)\n",
    "    if pi2.dim() == 1:\n",
    "        pi2, mu2, sg2 = pi2.unsqueeze(0), mu2.unsqueeze(0), sg2.unsqueeze(0)\n",
    "\n",
    "    B, K1 = pi1.shape\n",
    "    B2, K2 = pi2.shape\n",
    "\n",
    "    # Broadcast batch dimension if one is 1\n",
    "    if B != B2:\n",
    "        if B == 1:\n",
    "            pi1 = pi1.expand(B2, -1)\n",
    "            mu1 = mu1.expand(B2, -1)\n",
    "            sg1 = sg1.expand(B2, -1)\n",
    "            B = B2\n",
    "        elif B2 == 1:\n",
    "            pi2 = pi2.expand(B, -1)\n",
    "            mu2 = mu2.expand(B, -1)\n",
    "            sg2 = sg2.expand(B, -1)\n",
    "        else:\n",
    "            raise AssertionError(f\"Batch mismatch: {B} vs {B2}\")\n",
    "\n",
    "    # Cartesian product per batch\n",
    "    w = (pi1.unsqueeze(-1) * pi2.unsqueeze(-2)).reshape(B, K1 * K2)\n",
    "    mu = (mu1.unsqueeze(-1) + mu2.unsqueeze(-2)).reshape(B, K1 * K2)\n",
    "    var = ((sg1.unsqueeze(-1) ** 2) + (sg2.unsqueeze(-2) ** 2)).reshape(B, K1 * K2)\n",
    "    sg = torch.sqrt(var.clamp_min(eps))\n",
    "\n",
    "    # Threshold + renorm per batch row\n",
    "    w = w * (w > eps)\n",
    "    s = w.sum(dim=-1, keepdim=True)\n",
    "    w = torch.where(s > 0, w / s.clamp_min(eps), w)\n",
    "\n",
    "    # Return unbatched if both inputs were unbatched\n",
    "    if not batched:\n",
    "        return w.squeeze(0), mu.squeeze(0), sg.squeeze(0)\n",
    "    return w, mu, sg\n",
    "\n",
    "\n",
    "def sum_over_inputs_tree(pi, mu, sg, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Tree-reduce sum over input dimension I, batch-aware.\n",
    "\n",
    "    Inputs:\n",
    "      pi, mu, sg: (B, I, K) or (I, K)\n",
    "\n",
    "    Output:\n",
    "      pi_z, mu_z, sg_z: (B, Kout) or (Kout,)\n",
    "    \"\"\"\n",
    "    # Promote to batched if needed\n",
    "    batched = (pi.dim() == 3)\n",
    "    if pi.dim() == 2:\n",
    "        pi, mu, sg = pi.unsqueeze(0), mu.unsqueeze(0), sg.unsqueeze(0)\n",
    "\n",
    "    B, I, K = pi.shape\n",
    "\n",
    "    # Make a \"level\" list over i, each element is (B, K)\n",
    "    level = [(pi[:, i, :], mu[:, i, :], sg[:, i, :]) for i in range(I)]\n",
    "\n",
    "    while len(level) > 1:\n",
    "        new = []\n",
    "        for i in range(0, len(level), 2):\n",
    "            if i + 1 < len(level):\n",
    "                pi1, mu1, sg1 = level[i]\n",
    "                pi2, mu2, sg2 = level[i + 1]\n",
    "                new.append(gm_add(pi1, mu1, sg1, pi2, mu2, sg2, eps=eps))\n",
    "            else:\n",
    "                new.append(level[i])\n",
    "        level = new\n",
    "\n",
    "    pi_z, mu_z, sg_z = level[0]\n",
    "\n",
    "    if not batched:\n",
    "        return pi_z.squeeze(0), mu_z.squeeze(0), sg_z.squeeze(0)\n",
    "    return pi_z, mu_z, sg_z\n",
    "\n",
    "\n",
    "def preactivation_gm_for_neuron_tree(layer, j, pi_x, mu_x, sg_x, eps=1e-12):\n",
    "    \"\"\"\n",
    "    z_j = sum_i w_{j,i} x_i + b_j\n",
    "    Batch-aware if pi_x/mu_x/sg_x are (B, I, Kx).\n",
    "    \"\"\"\n",
    "    # product per input i\n",
    "    pi, mu, sigma = gm_product_per_neuron(\n",
    "        layer.pi_w[j], layer.mu_w[j], layer.sigma_w[j],\n",
    "        pi_x, mu_x, sg_x,\n",
    "        eps=eps, normalize_pi=True\n",
    "    )\n",
    "\n",
    "    # tree sum over i\n",
    "    pi_z, mu_z, sg_z = sum_over_inputs_tree(pi, mu, sigma, eps=eps)\n",
    "\n",
    "    # add bias mixture\n",
    "    if layer.bias:\n",
    "        pi_z, mu_z, sg_z = gm_add(\n",
    "            pi_z, mu_z, sg_z,\n",
    "            layer.pi_b[j], layer.mu_b[j], layer.sigma_b[j],\n",
    "            eps=eps\n",
    "        )\n",
    "\n",
    "    return pi_z, mu_z, sg_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "# Batch size 2, 3 inputs, 1 Gaussian each\n",
    "pi_x = torch.ones(2, 3, 1, device=device)        # deterministic mixture\n",
    "mu_x = torch.tensor([\n",
    "    [[1.0], [2.0], [3.0]],   # batch 0\n",
    "    [[-1.0], [0.5], [1.5]]   # batch 1\n",
    "], device=device)\n",
    "\n",
    "sigma_x = 0.01 * torch.ones_like(mu_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3759dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_z: tensor([[1.],\n",
      "        [1.]])\n",
      "mu_z: tensor([[4.6000],\n",
      "        [2.1000]])\n",
      "sg_z: tensor([[0.0500],\n",
      "        [0.0500]])\n"
     ]
    }
   ],
   "source": [
    "class FakeLayer:\n",
    "    def __init__(self, I, Kw=1, bias=True, device=\"cpu\"):\n",
    "        self.bias = bias\n",
    "\n",
    "        # weights: (neurons=1, I, Kw)\n",
    "        self.pi_w = torch.ones(1, I, Kw, device=device)\n",
    "        self.mu_w = torch.tensor(\n",
    "            [[[0.5], [-1.0], [2.0]]], device=device\n",
    "        )\n",
    "        self.sigma_w = 0.5 * torch.ones(1, I, Kw, device=device)\n",
    "\n",
    "        if bias:\n",
    "            self.pi_b = torch.ones(1, Kw, device=device)\n",
    "            self.mu_b = torch.tensor([[0.1]], device=device)\n",
    "            self.sigma_b = torch.tensor([[0.05]], device=device)\n",
    "\n",
    "layer = FakeLayer(I=3, Kw=1, bias=True, device=device)\n",
    "\n",
    "pi_z, mu_z, sg_z = preactivation_gm_for_neuron_tree(\n",
    "    layer,\n",
    "    j=0,\n",
    "    pi_x=pi_x,\n",
    "    mu_x=mu_x,\n",
    "    sg_x=sigma_x\n",
    ")\n",
    "\n",
    "print(\"pi_z:\", pi_z)\n",
    "print(\"mu_z:\", mu_z)\n",
    "print(\"sg_z:\", sg_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9a8c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights sum: 1.0\n",
      "all weights >= 0: True\n",
      "delta at zero weight (last comp): 0.3529192118399606 mu: 0.0 sigma: 0.0\n",
      "\n",
      "Monte Carlo vs mixture implied:\n",
      "P(Y=0): 0.3527345  vs  0.3529192118399606\n",
      "E[Y]:   0.7405637686712465  vs  0.74084416998383\n",
      "Var[Y]: 0.9248443516899084  vs  0.9265168107206846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "def truncate_0(pi, mu, sigma, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Distribution of Y = max(X, 0) when X is a Gaussian mixture.\n",
    "    Output is a mixture of:\n",
    "      - truncated normals on (0, +inf)\n",
    "      - plus a delta at 0 (represented as sigma=0 at mu=0)\n",
    "    \"\"\"\n",
    "    std = Normal(0.0, 1.0)\n",
    "    new_mixture = []\n",
    "    prob_zero = torch.tensor(0.0, device=pi.device)\n",
    "\n",
    "    for k in range(pi.numel()):\n",
    "        alpha = -mu[k] / sigma[k]  # threshold in standard-normal units\n",
    "\n",
    "        # P(X>0) = 1 - Phi(alpha)\n",
    "        prob_mass = 1 - std.cdf(alpha)\n",
    "\n",
    "        # NOTE: if prob_mass is extremely tiny, moments can blow up numerically.\n",
    "        prob_mass = prob_mass.clamp_min(eps)\n",
    "\n",
    "        # phi(alpha) is the standard normal pdf at alpha\n",
    "        phi = std.log_prob(alpha).exp()\n",
    "\n",
    "        # Truncated normal mean/var for truncation at 0 from below\n",
    "        ratio = phi / prob_mass\n",
    "        mu_trunc = mu[k] + sigma[k] * ratio\n",
    "        var_trunc = sigma[k]**2 * (1 + alpha * ratio - ratio**2)\n",
    "        sigma_trunc = torch.sqrt(var_trunc.clamp_min(eps))\n",
    "\n",
    "        pi_trunc = pi[k] * prob_mass\n",
    "        new_mixture.append((pi_trunc, mu_trunc, sigma_trunc))\n",
    "\n",
    "        # P(Y=0) accumulates the mass that was <=0\n",
    "        prob_zero = prob_zero + pi[k] * (1 - prob_mass)\n",
    "\n",
    "    # Add the delta at zero\n",
    "    new_mixture.append((prob_zero,\n",
    "                        torch.tensor(0.0, device=pi.device),\n",
    "                        torch.tensor(0.0, device=pi.device)))\n",
    "\n",
    "    pi_new = torch.stack([c[0] for c in new_mixture])\n",
    "    mu_new = torch.stack([c[1] for c in new_mixture])\n",
    "    sigma_new = torch.stack([c[2] for c in new_mixture])\n",
    "\n",
    "    # threshold + renorm\n",
    "    pi_new = pi_new * (pi_new > eps)\n",
    "    s = pi_new.sum(dim=-1, keepdim=True)\n",
    "    pi_new = torch.where(s > 0, pi_new / s.clamp_min(eps), pi_new)\n",
    "\n",
    "    return pi_new, mu_new, sigma_new\n",
    "\n",
    "\n",
    "# --- helpers to test correctness ---\n",
    "\n",
    "def sample_gmm(pi, mu, sigma, N, generator=None):\n",
    "    \"\"\"\n",
    "    Sample X from a 1D Gaussian mixture defined by (pi, mu, sigma).\n",
    "    Shapes: pi,mu,sigma = (K,)\n",
    "    Returns: (N,)\n",
    "    \"\"\"\n",
    "    K = pi.numel()\n",
    "    cat = torch.distributions.Categorical(probs=pi)\n",
    "    k = cat.sample((N,))  # component indices\n",
    "    eps = torch.randn(N, device=pi.device,)\n",
    "    return mu[k] + sigma[k] * eps\n",
    "\n",
    "\n",
    "def mixture_moments(pi, mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute mean and variance of a 1D Gaussian mixture.\n",
    "    Treats any sigma=0 component as a delta (still works).\n",
    "    \"\"\"\n",
    "    mean = (pi * mu).sum()\n",
    "    second = (pi * (sigma**2 + mu**2)).sum()\n",
    "    var = second - mean**2\n",
    "    return mean, var\n",
    "\n",
    "\n",
    "# --- a simple test you can run ---\n",
    "\n",
    "device = \"cpu\"\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# A small GMM (3 components) including negative/positive means (good stress test)\n",
    "pi = torch.tensor([0.2, 0.5, 0.3], device=device)\n",
    "mu = torch.tensor([-1.5, 0.2, 2.0], device=device)\n",
    "sigma = torch.tensor([0.7, 0.4, 0.8], device=device)\n",
    "\n",
    "# Compute truncated mixture\n",
    "pi_t, mu_t, sigma_t = truncate_0(pi, mu, sigma)\n",
    "\n",
    "# Sanity checks (must pass)\n",
    "print(\"weights sum:\", pi_t.sum().item())\n",
    "print(\"all weights >= 0:\", bool((pi_t >= 0).all()))\n",
    "print(\"delta at zero weight (last comp):\", pi_t[-1].item(), \"mu:\", mu_t[-1].item(), \"sigma:\", sigma_t[-1].item())\n",
    "\n",
    "# Compare moments to Monte Carlo\n",
    "N = 2_000_000\n",
    "\n",
    "x = sample_gmm(pi, mu, sigma, N)\n",
    "y = torch.clamp(x, min=0.0)  # Y = max(X,0)\n",
    "\n",
    "p0_mc = (y == 0).double().mean()\n",
    "mean_mc = y.mean()\n",
    "var_mc = y.var(unbiased=False)\n",
    "\n",
    "mean_mix, var_mix = mixture_moments(pi_t, mu_t, sigma_t)\n",
    "\n",
    "print(\"\\nMonte Carlo vs mixture implied:\")\n",
    "print(\"P(Y=0):\", p0_mc.item(), \" vs \", pi_t[-1].item())\n",
    "print(\"E[Y]:  \", mean_mc.item(), \" vs \", mean_mix.item())\n",
    "print(\"Var[Y]:\", var_mc.item(),  \" vs \", var_mix.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aefb227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5000, 0.5000]), tensor([0.7979, 0.0000]), tensor([0.6028, 0.0000]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_mixture = truncate_0(torch.tensor([1.]), torch.tensor([0.]), torch.tensor([1.]))\n",
    "\n",
    "relu_mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8c36148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) torch.Size([2, 4]) torch.Size([2, 4])\n",
      "delta weights: tensor([0.3529, 0.1346])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "def truncate_0_vectorized(pi, mu, sigma, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Vectorized + batch-friendly version.\n",
    "\n",
    "    Computes distribution of Y = max(X, 0) where X is a Gaussian mixture.\n",
    "\n",
    "    Inputs:\n",
    "      pi, mu, sigma: (..., K)\n",
    "    Returns:\n",
    "      pi_new, mu_new, sigma_new: (..., K+1)\n",
    "        - first K components: truncated normals on (0, +inf)\n",
    "        - last component: delta at 0 (mu=0, sigma=0)\n",
    "    \"\"\"\n",
    "    # Standard normal (broadcasts over tensors)\n",
    "    std = Normal(torch.tensor(0.0, device=pi.device, dtype=pi.dtype),\n",
    "                 torch.tensor(1.0, device=pi.device, dtype=pi.dtype))\n",
    "\n",
    "    # Avoid division by 0 in alpha\n",
    "    sigma_safe = sigma.clamp_min(eps)\n",
    "\n",
    "    # alpha = (0 - mu)/sigma = -mu/sigma\n",
    "    alpha = -mu / sigma_safe\n",
    "\n",
    "    # prob_mass = P(X > 0) = 1 - Phi(alpha)\n",
    "    prob_mass = (1.0 - std.cdf(alpha)).clamp_min(eps)\n",
    "\n",
    "    # phi(alpha) = standard normal pdf at alpha\n",
    "    phi = std.log_prob(alpha).exp()\n",
    "\n",
    "    # ratio = phi / prob_mass\n",
    "    ratio = phi / prob_mass\n",
    "\n",
    "    # Truncated component moments (lower truncation at 0)\n",
    "    mu_trunc = mu + sigma_safe * ratio\n",
    "    var_trunc = sigma_safe**2 * (1.0 + alpha * ratio - ratio**2)\n",
    "    sigma_trunc = torch.sqrt(var_trunc.clamp_min(eps))\n",
    "\n",
    "    # New weights for truncated components\n",
    "    pi_trunc = pi * prob_mass\n",
    "\n",
    "    # Mass that collapses to 0 (the \"clamped\" part)\n",
    "    prob_zero = (pi * (1.0 - prob_mass)).sum(dim=-1, keepdim=True)  # (..., 1)\n",
    "\n",
    "    # Append delta-at-zero as last component\n",
    "    pi_new = torch.cat([pi_trunc, prob_zero], dim=-1)               # (..., K+1)\n",
    "\n",
    "    zeros = torch.zeros_like(prob_zero)\n",
    "    mu_new = torch.cat([mu_trunc, zeros], dim=-1)                   # (..., K+1)\n",
    "    sigma_new = torch.cat([sigma_trunc, zeros], dim=-1)             # (..., K+1)\n",
    "\n",
    "    # Threshold + renormalize along last dim\n",
    "    pi_new = pi_new * (pi_new > eps)\n",
    "    s = pi_new.sum(dim=-1, keepdim=True)\n",
    "    pi_new = torch.where(s > 0, pi_new / s.clamp_min(eps), pi_new)\n",
    "\n",
    "    return pi_new, mu_new, sigma_new\n",
    "\n",
    "\n",
    "\n",
    "# Example batched test\n",
    "B, K = 2, 3\n",
    "pi = torch.tensor([[0.2, 0.5, 0.3],\n",
    "                   [0.1, 0.2, 0.7]], dtype=torch.float64)\n",
    "mu = torch.tensor([[-1.5, 0.2, 2.0],\n",
    "                   [-0.3, 0.1, 1.0]], dtype=torch.float64)\n",
    "sigma = torch.tensor([[0.7, 0.4, 0.8],\n",
    "                      [0.5, 0.2, 0.3]], dtype=torch.float64)\n",
    "\n",
    "pi_new, mu_new, sigma_new = truncate_0_vectorized(pi, mu, sigma)\n",
    "print(pi_new.shape, mu_new.shape, sigma_new.shape)  # (2, 4)\n",
    "print(\"delta weights:\", pi_new[:, -1])              # P(Y=0) per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ade0e70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU:\n",
      "pi_z: tensor([[1.],\n",
      "        [1.]], dtype=torch.float32)\n",
      "mu_z: tensor([[4.6000],\n",
      "        [2.1000]], dtype=torch.float32)\n",
      "sg_z: tensor([[0.0500],\n",
      "        [0.0500]], dtype=torch.float32)\n",
      "\n",
      "After ReLU:\n",
      "pi_a: tensor([[1., 0.],\n",
      "        [1., 0.]], dtype=torch.float32)\n",
      "mu_a: tensor([[4.6000, 0.0000],\n",
      "        [2.1000, 0.0000]], dtype=torch.float32)\n",
      "sg_a: tensor([[0.0500, 0.0000],\n",
      "        [0.0500, 0.0000]], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "pi_z, mu_z, sg_z = preactivation_gm_for_neuron_tree(\n",
    "    layer,\n",
    "    j=0,\n",
    "    pi_x=pi_x,\n",
    "    mu_x=mu_x,\n",
    "    sg_x=sigma_x\n",
    ")\n",
    "print(\"Before ReLU:\")\n",
    "print(\"pi_z:\", pi_z)\n",
    "print(\"mu_z:\", mu_z)\n",
    "print(\"sg_z:\", sg_z)\n",
    "pi_a, mu_a, sg_a = truncate_0_vectorized(pi_z, mu_z, sg_z)\n",
    "print(\"\\nAfter ReLU:\")\n",
    "print(\"pi_a:\", pi_a)\n",
    "print(\"mu_a:\", mu_a)\n",
    "print(\"sg_a:\", sg_a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
