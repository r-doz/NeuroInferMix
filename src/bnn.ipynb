{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78710e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from representation.bayesian_nn import BNN_GMM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8106953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "Non-zero components per batch sample: tensor([[2],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "#DATASET\n",
    "pi_x = torch.ones(2, 3, 1, device=device)        # deterministic mixture\n",
    "mu_x = torch.tensor([\n",
    "    [[1.0], [2.0], [3.0]],   # batch 0\n",
    "    [[-1.0], [0.5], [1.5]]   # batch 1\n",
    "], device=device)\n",
    "\n",
    "sigma_x = 0.01 * torch.ones_like(mu_x)\n",
    "\n",
    "#BNN FORWARD\n",
    "bnn = BNN_GMM(layer_sizes=[3, 10, 1], K=1, bias=True).to(device)\n",
    "\n",
    "pi_out, mu_out, sg_out = bnn(pi_x, mu_x, sigma_x, max_components=100)\n",
    "print(pi_out.shape)\n",
    "print(mu_out.shape)\n",
    "print(sg_out.shape)\n",
    "\n",
    "#count how many components have non-zero weight\n",
    "non_zero_components = (pi_out > 1e-12).sum(dim=2)\n",
    "print(\"Non-zero components per batch sample:\", non_zero_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0bb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_nll(pi, mu, sg, y, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood of targets y under a Gaussian mixture.\n",
    "    Shapes:\n",
    "      pi,mu,sg: (B, D, K)\n",
    "      y:        (B, D) or (B, D, 1)\n",
    "\n",
    "    Returns:\n",
    "      scalar loss\n",
    "    \"\"\"\n",
    "    if y.dim() == 2:\n",
    "        y = y.unsqueeze(-1)  # (B, D, 1)\n",
    "\n",
    "    sg = sg.clamp_min(eps)\n",
    "    logpi = torch.log(pi.clamp_min(eps))  # (B, D, K)\n",
    "\n",
    "    # log N(y | mu, sg)\n",
    "    log_norm = -0.5 * torch.log(2.0 * torch.pi * (sg ** 2))\n",
    "    log_exp = -0.5 * ((y - mu) / sg) ** 2\n",
    "    logp = log_norm + log_exp  # (B, D, K)\n",
    "\n",
    "    # logsumexp over components\n",
    "    log_mix = torch.logsumexp(logpi + logp, dim=-1)  # (B, D)\n",
    "    return (-log_mix).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e2fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def project_bnn_params_(bnn, eps=1e-12):\n",
    "    for layer in bnn.layers:\n",
    "        # keep sigmas positive\n",
    "        layer.sigma_w.clamp_(min=eps)\n",
    "        if layer.bias:\n",
    "            layer.sigma_b.clamp_(min=eps)\n",
    "\n",
    "        # keep pis valid probabilities\n",
    "        layer.pi_w.clamp_(min=0.0)\n",
    "        layer.pi_w.div_(layer.pi_w.sum(dim=-1, keepdim=True).clamp_min(eps))\n",
    "\n",
    "        if layer.bias:\n",
    "            layer.pi_b.clamp_(min=0.0)\n",
    "            layer.pi_b.div_(layer.pi_b.sum(dim=-1, keepdim=True).clamp_min(eps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943709cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bnn_gmm_regression(\n",
    "    bnn,\n",
    "    dataloader,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    eps=1e-12,\n",
    "    max_components=None,\n",
    "    last_relu=False,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    bnn.train()\n",
    "    opt = torch.optim.Adam(bnn.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Convert deterministic inputs x (B, d_in) to GMM input (B, d_in, 1)\n",
    "            pi_x = torch.ones(x.shape[0], x.shape[1], 1, device=device, dtype=x.dtype)\n",
    "            mu_x = x.unsqueeze(-1)\n",
    "            sg_x = 1e-2 * torch.ones_like(mu_x)  # choose your input noise\n",
    "\n",
    "            # Forward through mixture network\n",
    "            pi_out, mu_out, sg_out = bnn(\n",
    "                pi_x, mu_x, sg_x,\n",
    "                eps=eps,\n",
    "                last_relu=last_relu,\n",
    "                max_components=100\n",
    "            )\n",
    "\n",
    "            # Loss (regression NLL)\n",
    "            loss = gmm_nll(pi_out, mu_out, sg_out, y, eps=eps)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            project_bnn_params_(bnn, eps=eps)\n",
    "\n",
    "            total += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        print(f\"epoch {ep+1}/{epochs} | loss {total / max(1, n_batches):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a055ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20 | loss 226.037613\n",
      "epoch 2/20 | loss 56.191576\n",
      "epoch 3/20 | loss 40.072413\n",
      "epoch 4/20 | loss 32.063205\n",
      "epoch 5/20 | loss 27.285764\n",
      "epoch 6/20 | loss 24.485750\n",
      "epoch 7/20 | loss 22.284459\n",
      "epoch 8/20 | loss 20.475117\n",
      "epoch 9/20 | loss 19.025531\n",
      "epoch 10/20 | loss 17.837642\n",
      "epoch 11/20 | loss 16.763264\n",
      "epoch 12/20 | loss 15.797644\n",
      "epoch 13/20 | loss 14.897493\n",
      "epoch 14/20 | loss 14.049182\n",
      "epoch 15/20 | loss 13.282319\n",
      "epoch 16/20 | loss 12.586281\n",
      "epoch 17/20 | loss 11.913484\n",
      "epoch 18/20 | loss 11.274634\n",
      "epoch 19/20 | loss 10.740121\n",
      "epoch 20/20 | loss 10.250258\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Example: y = x0 - 2*x1 + 0.5*x2\n",
    "B = 512\n",
    "d_in = 3\n",
    "X = torch.randn(B, d_in)\n",
    "y = (X[:, 0] - 2.0 * X[:, 1] + 0.5 * X[:, 2]).unsqueeze(1)  # (B,1)\n",
    "\n",
    "ds = TensorDataset(X, y)\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bnn = BNN_GMM(layer_sizes=[3, 4, 1], K=1, bias=True).to(device)\n",
    "\n",
    "train_bnn_gmm_regression(bnn, dl, epochs=20, lr=1e-2, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# ---------- metrics helpers ----------\n",
    "\n",
    "def classification_accuracy_from_gmm_output(pi, mu, y_onehot):\n",
    "    \"\"\"\n",
    "    pi, mu: (B, 10, K)\n",
    "    y_onehot: (B, 10)\n",
    "    \"\"\"\n",
    "    # Mixture mean per class => (B,10)\n",
    "    y_hat = (pi * mu).sum(dim=-1)\n",
    "\n",
    "    pred = torch.argmax(y_hat, dim=1)          # (B,)\n",
    "    true = torch.argmax(y_onehot, dim=1)       # (B,)\n",
    "    return (pred == true).float().mean().item()\n",
    "\n",
    "def mixture_mean(pi, mu):\n",
    "    # pi, mu: (B, D, K) -> (B, D)\n",
    "    return (pi * mu).sum(dim=-1)\n",
    "\n",
    "def mixture_variance(pi, mu, sg, eps=1e-12):\n",
    "    mean = (pi * mu).sum(dim=-1)\n",
    "    second = (pi * (sg**2 + mu**2)).sum(dim=-1)\n",
    "    return second - mean**2\n",
    "\n",
    "def regression_metrics_from_output(pi, mu, sg, y, eps=1e-12):\n",
    "    \"\"\"\n",
    "    pi,mu,sg: (B,D,K)\n",
    "    y:        (B,D)\n",
    "    \"\"\"\n",
    "    y_hat = mixture_mean(pi, mu)\n",
    "    mse = ((y_hat - y) ** 2).mean()\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = (y_hat - y).abs().mean()\n",
    "\n",
    "    var = mixture_variance(pi, mu, sg, eps=eps)\n",
    "    std = torch.sqrt(var.clamp_min(eps))\n",
    "    cov_1sigma = (((y >= y_hat - std) & (y <= y_hat + std)).float().mean())\n",
    "\n",
    "    return {\n",
    "        \"MSE\": mse.item(),\n",
    "        \"RMSE\": rmse.item(),\n",
    "        \"MAE\": mae.item(),\n",
    "        \"Cov@1σ\": cov_1sigma.item(),\n",
    "    }\n",
    "\n",
    "def pad_gmm_to_K(pi, mu, sg, K_target, eps=1e-12):\n",
    "    B, D, K = pi.shape\n",
    "    if K == K_target:\n",
    "        return pi, mu, sg\n",
    "    if K > K_target:\n",
    "        # better not to happen if K_target = global max\n",
    "        pi = pi[:, :, :K_target]\n",
    "        mu = mu[:, :, :K_target]\n",
    "        sg = sg[:, :, :K_target]\n",
    "        s = pi.sum(dim=-1, keepdim=True)\n",
    "        pi = torch.where(s > eps, pi / s, pi)\n",
    "        return pi, mu, sg\n",
    "\n",
    "    pad = K_target - K\n",
    "    pi_pad = torch.zeros(B, D, pad, device=pi.device, dtype=pi.dtype)\n",
    "    mu_pad = torch.zeros(B, D, pad, device=mu.device, dtype=mu.dtype)\n",
    "    sg_pad = torch.ones(B, D, pad, device=sg.device, dtype=sg.dtype)\n",
    "    return (\n",
    "        torch.cat([pi, pi_pad], dim=-1),\n",
    "        torch.cat([mu, mu_pad], dim=-1),\n",
    "        torch.cat([sg, sg_pad], dim=-1),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_train_and_eval(\n",
    "    bnn,\n",
    "    X, y,\n",
    "    test_ratio=0.2,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    input_sigma=1e-2,\n",
    "    eps=1e-12,\n",
    "    last_relu=False,\n",
    "    device=None,\n",
    "    seed=0,\n",
    "    max_components=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes bnn + data, splits into train/test, trains, then evaluates on test at the end.\n",
    "\n",
    "    X: (N, d_in)\n",
    "    y: (N, d_out)\n",
    "    Returns: dict with test metrics (including NLL)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(bnn.parameters()).device\n",
    "\n",
    "    # Dataset + split\n",
    "    ds = TensorDataset(X, y)\n",
    "    n_total = len(ds)\n",
    "    n_test = int(round(test_ratio * n_total))\n",
    "    n_train = n_total - n_test\n",
    "\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "    ds_train, ds_test = random_split(ds, [n_train, n_test], generator=g)\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Optimizer\n",
    "    opt = torch.optim.Adam(bnn.parameters(), lr=lr)\n",
    "\n",
    "    # ---- train ----\n",
    "    bnn.train()\n",
    "    for ep in range(epochs):\n",
    "        running = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for xb, yb in dl_train:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            # Deterministic inputs -> GM inputs (B, d_in, 1)\n",
    "            pi_x = torch.ones(xb.shape[0], xb.shape[1], 1, device=device, dtype=xb.dtype)\n",
    "            mu_x = xb.unsqueeze(-1)\n",
    "            sg_x = input_sigma * torch.ones_like(mu_x)\n",
    "\n",
    "            pi_out, mu_out, sg_out = bnn(pi_x, mu_x, sg_x, eps=eps, last_relu=last_relu, max_components = max_components)\n",
    "            print(\"sg min/median:\", sg_out.min().item(), sg_out.median().item())\n",
    "\n",
    "            loss = gmm_nll(pi_out, mu_out, sg_out, yb, eps=eps)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            project_bnn_params_(bnn, eps=eps)\n",
    "\n",
    "            running += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "            print(f\"epoch {ep+1}/{epochs} | train NLL {running / max(1,n_batches):.6f}\")\n",
    "\n",
    "    # ---- test eval (only at end) ----\n",
    "    bnn.eval()\n",
    "    all_pi, all_mu, all_sg, all_y = [], [], [], []\n",
    "    K_max = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in dl_test:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            pi_x = torch.ones(xb.shape[0], xb.shape[1], 1, device=device, dtype=xb.dtype)\n",
    "            mu_x = xb.unsqueeze(-1)\n",
    "            sg_x = input_sigma * torch.ones_like(mu_x)\n",
    "\n",
    "            pi_out, mu_out, sg_out = bnn(\n",
    "                pi_x, mu_x, sg_x, eps=eps, last_relu=last_relu, max_components=max_components\n",
    "            )\n",
    "\n",
    "            K_max = max(K_max, pi_out.shape[-1])\n",
    "\n",
    "            all_pi.append(pi_out)\n",
    "            all_mu.append(mu_out)\n",
    "            all_sg.append(sg_out)\n",
    "            all_y.append(yb)\n",
    "\n",
    "    # pad to the largest K seen in the whole test set\n",
    "    all_pi2, all_mu2, all_sg2 = [], [], []\n",
    "    for pi_b, mu_b, sg_b in zip(all_pi, all_mu, all_sg):\n",
    "        pi_b, mu_b, sg_b = pad_gmm_to_K(pi_b, mu_b, sg_b, K_max, eps=eps)\n",
    "        all_pi2.append(pi_b)\n",
    "        all_mu2.append(mu_b)\n",
    "        all_sg2.append(sg_b)\n",
    "\n",
    "    pi_out = torch.cat(all_pi2, dim=0)\n",
    "    mu_out = torch.cat(all_mu2, dim=0)\n",
    "    sg_out = torch.cat(all_sg2, dim=0)\n",
    "    y_test = torch.cat(all_y, dim=0)\n",
    "\n",
    "\n",
    "    test_nll = gmm_nll(pi_out, mu_out, sg_out, y_test, eps=eps).item()\n",
    "    test_metrics = regression_metrics_from_output(pi_out, mu_out, sg_out, y_test, eps=eps)\n",
    "    test_metrics[\"NLL\"] = test_nll\n",
    "\n",
    "    acc = classification_accuracy_from_gmm_output(pi_out, mu_out, y_test)\n",
    "    test_metrics[\"Acc\"] = acc\n",
    "    print(\"TEST Acc:\", acc)\n",
    "\n",
    "\n",
    "    print(\"\\nTEST metrics:\", test_metrics)\n",
    "    return test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec78df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/30 | train NLL 8.870159\n",
      "epoch 1/30 | train NLL 8.565257\n",
      "epoch 1/30 | train NLL 8.478438\n",
      "epoch 1/30 | train NLL 7.542206\n",
      "epoch 1/30 | train NLL 7.145145\n",
      "epoch 1/30 | train NLL 7.034742\n",
      "epoch 1/30 | train NLL 6.804343\n",
      "epoch 2/30 | train NLL 4.109103\n",
      "epoch 2/30 | train NLL 3.626495\n",
      "epoch 2/30 | train NLL 3.439876\n",
      "epoch 2/30 | train NLL 3.211209\n",
      "epoch 2/30 | train NLL 3.215924\n",
      "epoch 2/30 | train NLL 3.187468\n",
      "epoch 2/30 | train NLL 2.974972\n",
      "epoch 3/30 | train NLL 2.734701\n",
      "epoch 3/30 | train NLL 2.306779\n",
      "epoch 3/30 | train NLL 2.163274\n",
      "epoch 3/30 | train NLL 2.068856\n",
      "epoch 3/30 | train NLL 2.095836\n",
      "epoch 3/30 | train NLL 2.041606\n",
      "epoch 3/30 | train NLL 1.972659\n",
      "epoch 4/30 | train NLL 1.836417\n",
      "epoch 4/30 | train NLL 1.836692\n",
      "epoch 4/30 | train NLL 1.766028\n",
      "epoch 4/30 | train NLL 1.734610\n",
      "epoch 4/30 | train NLL 1.733114\n",
      "epoch 4/30 | train NLL 1.666056\n",
      "epoch 4/30 | train NLL 1.595639\n",
      "epoch 5/30 | train NLL 1.383654\n",
      "epoch 5/30 | train NLL 1.468257\n",
      "epoch 5/30 | train NLL 1.435868\n",
      "epoch 5/30 | train NLL 1.485323\n",
      "epoch 5/30 | train NLL 1.477893\n",
      "epoch 5/30 | train NLL 1.486648\n",
      "epoch 5/30 | train NLL 1.444772\n",
      "epoch 6/30 | train NLL 1.368504\n",
      "epoch 6/30 | train NLL 1.371883\n",
      "epoch 6/30 | train NLL 1.348795\n",
      "epoch 6/30 | train NLL 1.363809\n",
      "epoch 6/30 | train NLL 1.370843\n",
      "epoch 6/30 | train NLL 1.395725\n",
      "epoch 6/30 | train NLL 1.380408\n",
      "epoch 7/30 | train NLL 1.390102\n",
      "epoch 7/30 | train NLL 1.312945\n",
      "epoch 7/30 | train NLL 1.341623\n",
      "epoch 7/30 | train NLL 1.376312\n",
      "epoch 7/30 | train NLL 1.359580\n",
      "epoch 7/30 | train NLL 1.346098\n",
      "epoch 7/30 | train NLL 1.328751\n",
      "epoch 8/30 | train NLL 1.367457\n",
      "epoch 8/30 | train NLL 1.282452\n",
      "epoch 8/30 | train NLL 1.247235\n",
      "epoch 8/30 | train NLL 1.245259\n",
      "epoch 8/30 | train NLL 1.259381\n",
      "epoch 8/30 | train NLL 1.289553\n",
      "epoch 8/30 | train NLL 1.304981\n",
      "epoch 9/30 | train NLL 1.225103\n",
      "epoch 9/30 | train NLL 1.256422\n",
      "epoch 9/30 | train NLL 1.301603\n",
      "epoch 9/30 | train NLL 1.266214\n",
      "epoch 9/30 | train NLL 1.251770\n",
      "epoch 9/30 | train NLL 1.260345\n",
      "epoch 9/30 | train NLL 1.264010\n",
      "epoch 10/30 | train NLL 1.155813\n",
      "epoch 10/30 | train NLL 1.229702\n",
      "epoch 10/30 | train NLL 1.243517\n",
      "epoch 10/30 | train NLL 1.226538\n",
      "epoch 10/30 | train NLL 1.232969\n",
      "epoch 10/30 | train NLL 1.230628\n",
      "epoch 10/30 | train NLL 1.224127\n",
      "epoch 11/30 | train NLL 1.281868\n",
      "epoch 11/30 | train NLL 1.316222\n",
      "epoch 11/30 | train NLL 1.251531\n",
      "epoch 11/30 | train NLL 1.214915\n",
      "epoch 11/30 | train NLL 1.217662\n",
      "epoch 11/30 | train NLL 1.203851\n",
      "epoch 11/30 | train NLL 1.183075\n",
      "epoch 12/30 | train NLL 1.175824\n",
      "epoch 12/30 | train NLL 1.165168\n",
      "epoch 12/30 | train NLL 1.182169\n",
      "epoch 12/30 | train NLL 1.194110\n",
      "epoch 12/30 | train NLL 1.168934\n",
      "epoch 12/30 | train NLL 1.170574\n",
      "epoch 12/30 | train NLL 1.146446\n",
      "epoch 13/30 | train NLL 1.115246\n",
      "epoch 13/30 | train NLL 1.097053\n",
      "epoch 13/30 | train NLL 1.115381\n",
      "epoch 13/30 | train NLL 1.128017\n",
      "epoch 13/30 | train NLL 1.129062\n",
      "epoch 13/30 | train NLL 1.125873\n",
      "epoch 13/30 | train NLL 1.121888\n",
      "epoch 14/30 | train NLL 1.129782\n",
      "epoch 14/30 | train NLL 1.102175\n",
      "epoch 14/30 | train NLL 1.104145\n",
      "epoch 14/30 | train NLL 1.083520\n",
      "epoch 14/30 | train NLL 1.077302\n",
      "epoch 14/30 | train NLL 1.089536\n",
      "epoch 14/30 | train NLL 1.085315\n",
      "epoch 15/30 | train NLL 1.094305\n",
      "epoch 15/30 | train NLL 1.086284\n",
      "epoch 15/30 | train NLL 1.069700\n",
      "epoch 15/30 | train NLL 1.067566\n",
      "epoch 15/30 | train NLL 1.048920\n",
      "epoch 15/30 | train NLL 1.051989\n",
      "epoch 15/30 | train NLL 1.043191\n",
      "epoch 16/30 | train NLL 1.023550\n",
      "epoch 16/30 | train NLL 1.026183\n",
      "epoch 16/30 | train NLL 1.000077\n",
      "epoch 16/30 | train NLL 1.005979\n",
      "epoch 16/30 | train NLL 1.012013\n",
      "epoch 16/30 | train NLL 1.015890\n",
      "epoch 16/30 | train NLL 0.994083\n",
      "epoch 17/30 | train NLL 1.005793\n",
      "epoch 17/30 | train NLL 0.970840\n",
      "epoch 17/30 | train NLL 0.972240\n",
      "epoch 17/30 | train NLL 0.972108\n",
      "epoch 17/30 | train NLL 0.975967\n",
      "epoch 17/30 | train NLL 0.958130\n",
      "epoch 17/30 | train NLL 0.965078\n",
      "epoch 18/30 | train NLL 0.872444\n",
      "epoch 18/30 | train NLL 0.871708\n",
      "epoch 18/30 | train NLL 0.898767\n",
      "epoch 18/30 | train NLL 0.887195\n",
      "epoch 18/30 | train NLL 0.898682\n",
      "epoch 18/30 | train NLL 0.901769\n",
      "epoch 18/30 | train NLL 0.928180\n",
      "epoch 19/30 | train NLL 0.931375\n",
      "epoch 19/30 | train NLL 0.962154\n",
      "epoch 19/30 | train NLL 0.914355\n",
      "epoch 19/30 | train NLL 0.885116\n",
      "epoch 19/30 | train NLL 0.880226\n",
      "epoch 19/30 | train NLL 0.861478\n",
      "epoch 19/30 | train NLL 0.862679\n",
      "epoch 20/30 | train NLL 0.806837\n",
      "epoch 20/30 | train NLL 0.800609\n",
      "epoch 20/30 | train NLL 0.801956\n",
      "epoch 20/30 | train NLL 0.812740\n",
      "epoch 20/30 | train NLL 0.799840\n",
      "epoch 20/30 | train NLL 0.812742\n",
      "epoch 20/30 | train NLL 0.798793\n",
      "epoch 21/30 | train NLL 0.876780\n",
      "epoch 21/30 | train NLL 0.820282\n",
      "epoch 21/30 | train NLL 0.775928\n",
      "epoch 21/30 | train NLL 0.770306\n",
      "epoch 21/30 | train NLL 0.764077\n",
      "epoch 21/30 | train NLL 0.760787\n",
      "epoch 21/30 | train NLL 0.728863\n",
      "epoch 22/30 | train NLL 0.668901\n",
      "epoch 22/30 | train NLL 0.624941\n",
      "epoch 22/30 | train NLL 0.646759\n",
      "epoch 22/30 | train NLL 0.658061\n",
      "epoch 22/30 | train NLL 0.666992\n",
      "epoch 22/30 | train NLL 0.679975\n",
      "epoch 22/30 | train NLL 0.689493\n",
      "epoch 23/30 | train NLL 0.606547\n",
      "epoch 23/30 | train NLL 0.603518\n",
      "epoch 23/30 | train NLL 0.613902\n",
      "epoch 23/30 | train NLL 0.614645\n",
      "epoch 23/30 | train NLL 0.620088\n",
      "epoch 23/30 | train NLL 0.615385\n",
      "epoch 23/30 | train NLL 0.616669\n",
      "epoch 24/30 | train NLL 0.566245\n",
      "epoch 24/30 | train NLL 0.525805\n",
      "epoch 24/30 | train NLL 0.529841\n",
      "epoch 24/30 | train NLL 0.519550\n",
      "epoch 24/30 | train NLL 0.531893\n",
      "epoch 24/30 | train NLL 0.536693\n",
      "epoch 24/30 | train NLL 0.555387\n",
      "epoch 25/30 | train NLL 0.518932\n",
      "epoch 25/30 | train NLL 0.479867\n",
      "epoch 25/30 | train NLL 0.491598\n",
      "epoch 25/30 | train NLL 0.485758\n",
      "epoch 25/30 | train NLL 0.483856\n",
      "epoch 25/30 | train NLL 0.478448\n",
      "epoch 25/30 | train NLL 0.458086\n",
      "epoch 26/30 | train NLL 0.416948\n",
      "epoch 26/30 | train NLL 0.450771\n",
      "epoch 26/30 | train NLL 0.450105\n",
      "epoch 26/30 | train NLL 0.409893\n",
      "epoch 26/30 | train NLL 0.399079\n",
      "epoch 26/30 | train NLL 0.386137\n",
      "epoch 26/30 | train NLL 0.395694\n",
      "epoch 27/30 | train NLL 0.250083\n",
      "epoch 27/30 | train NLL 0.224493\n",
      "epoch 27/30 | train NLL 0.245462\n",
      "epoch 27/30 | train NLL 0.265177\n",
      "epoch 27/30 | train NLL 0.281313\n",
      "epoch 27/30 | train NLL 0.295229\n",
      "epoch 27/30 | train NLL 0.316485\n",
      "epoch 28/30 | train NLL 0.344539\n",
      "epoch 28/30 | train NLL 0.247405\n",
      "epoch 28/30 | train NLL 0.248019\n",
      "epoch 28/30 | train NLL 0.257820\n",
      "epoch 28/30 | train NLL 0.228817\n",
      "epoch 28/30 | train NLL 0.218475\n",
      "epoch 28/30 | train NLL 0.202705\n",
      "epoch 29/30 | train NLL 0.166569\n",
      "epoch 29/30 | train NLL 0.195242\n",
      "epoch 29/30 | train NLL 0.195342\n",
      "epoch 29/30 | train NLL 0.197819\n",
      "epoch 29/30 | train NLL 0.161179\n",
      "epoch 29/30 | train NLL 0.123047\n",
      "epoch 29/30 | train NLL 0.095289\n",
      "epoch 30/30 | train NLL 0.090112\n",
      "epoch 30/30 | train NLL 0.069228\n",
      "epoch 30/30 | train NLL 0.083694\n",
      "epoch 30/30 | train NLL 0.012245\n",
      "epoch 30/30 | train NLL -0.008270\n",
      "epoch 30/30 | train NLL 0.005979\n",
      "epoch 30/30 | train NLL 0.010134\n",
      "TEST Acc: 1.0\n",
      "\n",
      "TEST metrics: {'MSE': 0.33661577105522156, 'RMSE': 0.5801859498023987, 'MAE': 0.32572129368782043, 'Cov@1σ': 0.7745097875595093, 'NLL': -0.015417061746120453, 'Acc': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# X: (N,3), y: (N,1)\n",
    "metrics = train_test_split_train_and_eval(\n",
    "    bnn, X, y,\n",
    "    test_ratio=0.2,\n",
    "    epochs=30,\n",
    "    lr=1e-2,\n",
    "    batch_size=64,\n",
    "    input_sigma=1e-2,\n",
    "    max_components=100,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bada8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def load_mnist_onehot_tensors(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    n_max=None,\n",
    "    normalize=True,\n",
    "    device=\"cpu\",\n",
    "    dtype=torch.float32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X: (N, 784) float\n",
    "      y: (N, 10)  one-hot float\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),  # standard MNIST stats\n",
    "        ])\n",
    "    else:\n",
    "        tfm = transforms.ToTensor()\n",
    "\n",
    "    ds = datasets.MNIST(root=root, train=train, download=True, transform=tfm)\n",
    "\n",
    "    if n_max is None:\n",
    "        n_max = len(ds)\n",
    "    n_max = min(n_max, len(ds))\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(n_max):\n",
    "        x_i, y_i = ds[i]              # x_i: (1,28,28), y_i: int in [0..9]\n",
    "        X_list.append(x_i.view(-1))   # -> (784,)\n",
    "        y_list.append(int(y_i))\n",
    "\n",
    "    X = torch.stack(X_list, dim=0).to(device=device, dtype=dtype)  # (N,784)\n",
    "\n",
    "    y_idx = torch.tensor(y_list, device=device, dtype=torch.long)  # (N,)\n",
    "    y = torch.zeros((n_max, 10), device=device, dtype=dtype)\n",
    "    y.scatter_(1, y_idx.unsqueeze(1), 1.0)                          # (N,10) one-hot\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def run_mnist_classification_with_your_trainer(\n",
    "    bnn,\n",
    "    device=None,\n",
    "    root=\"./data\",\n",
    "    n_max_train=1000,\n",
    "    test_ratio=0.2,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    input_sigma=1e-2,\n",
    "    eps=1e-12,\n",
    "    last_relu=False,\n",
    "    seed=0,\n",
    "    normalize=True,\n",
    "    max_components=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = next(bnn.parameters()).device\n",
    "    bnn.to(device)\n",
    "\n",
    "    # Load MNIST train split (we'll do train/test split inside your function)\n",
    "    X, y = load_mnist_onehot_tensors(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        n_max=n_max_train,\n",
    "        normalize=normalize,\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    # Train + final test metrics (your function)\n",
    "    test_metrics = train_test_split_train_and_eval(\n",
    "        bnn=bnn,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        test_ratio=test_ratio,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        input_sigma=input_sigma,\n",
    "        eps=eps,\n",
    "        last_relu=last_relu,\n",
    "        device=device,\n",
    "        seed=seed,\n",
    "        max_components=max_components,\n",
    "    )\n",
    "\n",
    "    return test_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b514383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10 | train NLL 61.594402\n",
      "epoch 1/10 | train NLL 62.799814\n",
      "epoch 1/10 | train NLL 67.541935\n",
      "epoch 1/10 | train NLL 72.295737\n",
      "epoch 1/10 | train NLL 74.231345\n",
      "epoch 1/10 | train NLL 77.124811\n",
      "epoch 1/10 | train NLL 77.774408\n",
      "epoch 1/10 | train NLL 77.481841\n",
      "epoch 1/10 | train NLL 77.096055\n",
      "epoch 1/10 | train NLL 75.528369\n",
      "epoch 1/10 | train NLL 75.227519\n",
      "epoch 1/10 | train NLL 75.122584\n",
      "epoch 1/10 | train NLL 74.673053\n",
      "epoch 1/10 | train NLL 74.488170\n",
      "epoch 1/10 | train NLL 74.376057\n",
      "epoch 1/10 | train NLL 73.617378\n",
      "epoch 1/10 | train NLL 72.905398\n",
      "epoch 1/10 | train NLL 71.687535\n",
      "epoch 1/10 | train NLL 70.749522\n",
      "epoch 1/10 | train NLL 70.066523\n",
      "epoch 1/10 | train NLL 68.918155\n",
      "epoch 1/10 | train NLL 67.943067\n",
      "epoch 1/10 | train NLL 67.215976\n",
      "epoch 1/10 | train NLL 66.552923\n",
      "epoch 1/10 | train NLL 65.830845\n",
      "epoch 1/10 | train NLL 65.368674\n",
      "epoch 1/10 | train NLL 64.725141\n",
      "epoch 1/10 | train NLL 64.287114\n",
      "epoch 1/10 | train NLL 63.507536\n",
      "epoch 1/10 | train NLL 62.576983\n",
      "epoch 1/10 | train NLL 62.107601\n",
      "epoch 1/10 | train NLL 61.438493\n",
      "epoch 1/10 | train NLL 60.791683\n",
      "epoch 1/10 | train NLL 60.077742\n",
      "epoch 1/10 | train NLL 59.534627\n",
      "epoch 1/10 | train NLL 58.897146\n",
      "epoch 1/10 | train NLL 58.200735\n",
      "epoch 1/10 | train NLL 57.708828\n",
      "epoch 1/10 | train NLL 57.141417\n",
      "epoch 1/10 | train NLL 56.604216\n",
      "epoch 1/10 | train NLL 56.092674\n",
      "epoch 1/10 | train NLL 55.492912\n",
      "epoch 1/10 | train NLL 54.902373\n",
      "epoch 1/10 | train NLL 54.251712\n",
      "epoch 1/10 | train NLL 53.540555\n",
      "epoch 1/10 | train NLL 52.957756\n",
      "epoch 1/10 | train NLL 52.342189\n",
      "epoch 1/10 | train NLL 51.770268\n",
      "epoch 1/10 | train NLL 51.253798\n",
      "epoch 1/10 | train NLL 50.801985\n",
      "epoch 1/10 | train NLL 50.390503\n",
      "epoch 1/10 | train NLL 49.920584\n",
      "epoch 1/10 | train NLL 49.398300\n",
      "epoch 1/10 | train NLL 48.874996\n",
      "epoch 1/10 | train NLL 48.417110\n",
      "epoch 1/10 | train NLL 48.018859\n",
      "epoch 1/10 | train NLL 47.474157\n",
      "epoch 1/10 | train NLL 47.048804\n",
      "epoch 1/10 | train NLL 46.629680\n",
      "epoch 1/10 | train NLL 46.193358\n",
      "epoch 1/10 | train NLL 45.723594\n",
      "epoch 1/10 | train NLL 45.330320\n",
      "epoch 1/10 | train NLL 44.937566\n",
      "epoch 1/10 | train NLL 44.500679\n",
      "epoch 1/10 | train NLL 44.156547\n",
      "epoch 1/10 | train NLL 43.739725\n",
      "epoch 1/10 | train NLL 43.342169\n",
      "epoch 2/10 | train NLL 22.401363\n",
      "epoch 2/10 | train NLL 20.419003\n",
      "epoch 2/10 | train NLL 18.528474\n",
      "epoch 2/10 | train NLL 18.997555\n",
      "epoch 2/10 | train NLL 18.428995\n",
      "epoch 2/10 | train NLL 18.611982\n",
      "epoch 2/10 | train NLL 18.093945\n",
      "epoch 2/10 | train NLL 17.979617\n",
      "epoch 2/10 | train NLL 17.156788\n",
      "epoch 2/10 | train NLL 16.632766\n",
      "epoch 2/10 | train NLL 16.453571\n",
      "epoch 2/10 | train NLL 16.270042\n",
      "epoch 2/10 | train NLL 16.081331\n",
      "epoch 2/10 | train NLL 15.827429\n",
      "epoch 2/10 | train NLL 15.529521\n",
      "epoch 2/10 | train NLL 15.307090\n",
      "epoch 2/10 | train NLL 15.042686\n",
      "epoch 2/10 | train NLL 14.962937\n",
      "epoch 2/10 | train NLL 14.710342\n",
      "epoch 2/10 | train NLL 14.507635\n",
      "epoch 2/10 | train NLL 14.321845\n",
      "epoch 2/10 | train NLL 14.138911\n",
      "epoch 2/10 | train NLL 14.076486\n",
      "epoch 2/10 | train NLL 13.935377\n",
      "epoch 2/10 | train NLL 13.920968\n",
      "epoch 2/10 | train NLL 13.717349\n",
      "epoch 2/10 | train NLL 13.605195\n",
      "epoch 2/10 | train NLL 13.560678\n",
      "epoch 2/10 | train NLL 13.470408\n",
      "epoch 2/10 | train NLL 13.399249\n",
      "epoch 2/10 | train NLL 13.326345\n",
      "epoch 2/10 | train NLL 13.288836\n",
      "epoch 2/10 | train NLL 13.196524\n",
      "epoch 2/10 | train NLL 13.128611\n",
      "epoch 2/10 | train NLL 13.103550\n",
      "epoch 2/10 | train NLL 13.050164\n",
      "epoch 2/10 | train NLL 12.972756\n",
      "epoch 2/10 | train NLL 12.890600\n",
      "epoch 2/10 | train NLL 12.803990\n",
      "epoch 2/10 | train NLL 12.744348\n",
      "epoch 2/10 | train NLL 12.666420\n",
      "epoch 2/10 | train NLL 12.633877\n",
      "epoch 2/10 | train NLL 12.562357\n",
      "epoch 2/10 | train NLL 12.503038\n",
      "epoch 2/10 | train NLL 12.429959\n",
      "epoch 2/10 | train NLL 12.387208\n",
      "epoch 2/10 | train NLL 12.280623\n",
      "epoch 2/10 | train NLL 12.202541\n",
      "epoch 2/10 | train NLL 12.125463\n",
      "epoch 2/10 | train NLL 12.055023\n",
      "epoch 2/10 | train NLL 11.990469\n",
      "epoch 2/10 | train NLL 11.940802\n",
      "epoch 2/10 | train NLL 11.868575\n",
      "epoch 2/10 | train NLL 11.816539\n",
      "epoch 2/10 | train NLL 11.759637\n",
      "epoch 2/10 | train NLL 11.681603\n",
      "epoch 2/10 | train NLL 11.644689\n",
      "epoch 2/10 | train NLL 11.574695\n",
      "epoch 2/10 | train NLL 11.509802\n",
      "epoch 2/10 | train NLL 11.441759\n",
      "epoch 2/10 | train NLL 11.380041\n",
      "epoch 2/10 | train NLL 11.310308\n",
      "epoch 2/10 | train NLL 11.263399\n",
      "epoch 2/10 | train NLL 11.209793\n",
      "epoch 2/10 | train NLL 11.131575\n",
      "epoch 2/10 | train NLL 11.077883\n",
      "epoch 2/10 | train NLL 11.050925\n",
      "epoch 3/10 | train NLL 7.250998\n",
      "epoch 3/10 | train NLL 7.051832\n",
      "epoch 3/10 | train NLL 7.176178\n",
      "epoch 3/10 | train NLL 7.068251\n",
      "epoch 3/10 | train NLL 7.393131\n",
      "epoch 3/10 | train NLL 7.272977\n",
      "epoch 3/10 | train NLL 7.387767\n",
      "epoch 3/10 | train NLL 7.478581\n",
      "epoch 3/10 | train NLL 7.451980\n",
      "epoch 3/10 | train NLL 7.291696\n",
      "epoch 3/10 | train NLL 7.358002\n",
      "epoch 3/10 | train NLL 7.346364\n",
      "epoch 3/10 | train NLL 7.306634\n",
      "epoch 3/10 | train NLL 7.279528\n",
      "epoch 3/10 | train NLL 7.300283\n",
      "epoch 3/10 | train NLL 7.280198\n",
      "epoch 3/10 | train NLL 7.309228\n",
      "epoch 3/10 | train NLL 7.269027\n",
      "epoch 3/10 | train NLL 7.313574\n",
      "epoch 3/10 | train NLL 7.263149\n",
      "epoch 3/10 | train NLL 7.218897\n",
      "epoch 3/10 | train NLL 7.163368\n",
      "epoch 3/10 | train NLL 7.104485\n",
      "epoch 3/10 | train NLL 7.088871\n",
      "epoch 3/10 | train NLL 7.066090\n",
      "epoch 3/10 | train NLL 7.031552\n",
      "epoch 3/10 | train NLL 6.982641\n",
      "epoch 3/10 | train NLL 6.942171\n",
      "epoch 3/10 | train NLL 6.927219\n",
      "epoch 3/10 | train NLL 6.868095\n",
      "epoch 3/10 | train NLL 6.854884\n",
      "epoch 3/10 | train NLL 6.815208\n",
      "epoch 3/10 | train NLL 6.773387\n",
      "epoch 3/10 | train NLL 6.749944\n",
      "epoch 3/10 | train NLL 6.755978\n",
      "epoch 3/10 | train NLL 6.724611\n",
      "epoch 3/10 | train NLL 6.688287\n",
      "epoch 3/10 | train NLL 6.698515\n",
      "epoch 3/10 | train NLL 6.675956\n",
      "epoch 3/10 | train NLL 6.657634\n",
      "epoch 3/10 | train NLL 6.642517\n",
      "epoch 3/10 | train NLL 6.622347\n",
      "epoch 3/10 | train NLL 6.589654\n",
      "epoch 3/10 | train NLL 6.562016\n",
      "epoch 3/10 | train NLL 6.533581\n",
      "epoch 3/10 | train NLL 6.533299\n",
      "epoch 3/10 | train NLL 6.523029\n",
      "epoch 3/10 | train NLL 6.503041\n",
      "epoch 3/10 | train NLL 6.494069\n",
      "epoch 3/10 | train NLL 6.474855\n",
      "epoch 3/10 | train NLL 6.457588\n",
      "epoch 3/10 | train NLL 6.450277\n",
      "epoch 3/10 | train NLL 6.428040\n",
      "epoch 3/10 | train NLL 6.433836\n",
      "epoch 3/10 | train NLL 6.426389\n",
      "epoch 3/10 | train NLL 6.401828\n",
      "epoch 3/10 | train NLL 6.397715\n",
      "epoch 3/10 | train NLL 6.383144\n",
      "epoch 3/10 | train NLL 6.354124\n",
      "epoch 3/10 | train NLL 6.347243\n",
      "epoch 3/10 | train NLL 6.318183\n",
      "epoch 3/10 | train NLL 6.308793\n",
      "epoch 3/10 | train NLL 6.291866\n",
      "epoch 3/10 | train NLL 6.275700\n",
      "epoch 3/10 | train NLL 6.264397\n",
      "epoch 3/10 | train NLL 6.243031\n",
      "epoch 3/10 | train NLL 6.222077\n",
      "epoch 4/10 | train NLL 4.538193\n",
      "epoch 4/10 | train NLL 4.903524\n",
      "epoch 4/10 | train NLL 4.809650\n",
      "epoch 4/10 | train NLL 4.840884\n",
      "epoch 4/10 | train NLL 4.803214\n",
      "epoch 4/10 | train NLL 4.774452\n",
      "epoch 4/10 | train NLL 4.682153\n",
      "epoch 4/10 | train NLL 4.681072\n",
      "epoch 4/10 | train NLL 4.745021\n",
      "epoch 4/10 | train NLL 4.720505\n",
      "epoch 4/10 | train NLL 4.733288\n",
      "epoch 4/10 | train NLL 4.699746\n",
      "epoch 4/10 | train NLL 4.692852\n",
      "epoch 4/10 | train NLL 4.634826\n",
      "epoch 4/10 | train NLL 4.677932\n",
      "epoch 4/10 | train NLL 4.694154\n",
      "epoch 4/10 | train NLL 4.683762\n",
      "epoch 4/10 | train NLL 4.690858\n",
      "epoch 4/10 | train NLL 4.687140\n",
      "epoch 4/10 | train NLL 4.683038\n",
      "epoch 4/10 | train NLL 4.659518\n",
      "epoch 4/10 | train NLL 4.634797\n",
      "epoch 4/10 | train NLL 4.647246\n",
      "epoch 4/10 | train NLL 4.650070\n",
      "epoch 4/10 | train NLL 4.640324\n",
      "epoch 4/10 | train NLL 4.626376\n",
      "epoch 4/10 | train NLL 4.611303\n",
      "epoch 4/10 | train NLL 4.625062\n",
      "epoch 4/10 | train NLL 4.631044\n",
      "epoch 4/10 | train NLL 4.611946\n",
      "epoch 4/10 | train NLL 4.600360\n",
      "epoch 4/10 | train NLL 4.584083\n",
      "epoch 4/10 | train NLL 4.559081\n",
      "epoch 4/10 | train NLL 4.546301\n",
      "epoch 4/10 | train NLL 4.533195\n",
      "epoch 4/10 | train NLL 4.524721\n",
      "epoch 4/10 | train NLL 4.504270\n",
      "epoch 4/10 | train NLL 4.502483\n",
      "epoch 4/10 | train NLL 4.497369\n",
      "epoch 4/10 | train NLL 4.481216\n",
      "epoch 4/10 | train NLL 4.482397\n",
      "epoch 4/10 | train NLL 4.488047\n",
      "epoch 4/10 | train NLL 4.486847\n",
      "epoch 4/10 | train NLL 4.466912\n",
      "epoch 4/10 | train NLL 4.462665\n",
      "epoch 4/10 | train NLL 4.425654\n",
      "epoch 4/10 | train NLL 4.404185\n",
      "epoch 4/10 | train NLL 4.418801\n",
      "epoch 4/10 | train NLL 4.399070\n",
      "epoch 4/10 | train NLL 4.405235\n",
      "epoch 4/10 | train NLL 4.400568\n",
      "epoch 4/10 | train NLL 4.386979\n",
      "epoch 4/10 | train NLL 4.378596\n",
      "epoch 4/10 | train NLL 4.390791\n",
      "epoch 4/10 | train NLL 4.369929\n",
      "epoch 4/10 | train NLL 4.356342\n",
      "epoch 4/10 | train NLL 4.356639\n",
      "epoch 4/10 | train NLL 4.365409\n",
      "epoch 4/10 | train NLL 4.356918\n",
      "epoch 4/10 | train NLL 4.360195\n",
      "epoch 4/10 | train NLL 4.351898\n",
      "epoch 4/10 | train NLL 4.343992\n",
      "epoch 4/10 | train NLL 4.339602\n",
      "epoch 4/10 | train NLL 4.331741\n",
      "epoch 4/10 | train NLL 4.322543\n",
      "epoch 4/10 | train NLL 4.318483\n",
      "epoch 4/10 | train NLL 4.308167\n",
      "epoch 5/10 | train NLL 4.835706\n",
      "epoch 5/10 | train NLL 4.587835\n",
      "epoch 5/10 | train NLL 4.490473\n",
      "epoch 5/10 | train NLL 4.314706\n",
      "epoch 5/10 | train NLL 4.155622\n",
      "epoch 5/10 | train NLL 3.915762\n",
      "epoch 5/10 | train NLL 3.890082\n",
      "epoch 5/10 | train NLL 3.884469\n",
      "epoch 5/10 | train NLL 3.791961\n",
      "epoch 5/10 | train NLL 3.698101\n",
      "epoch 5/10 | train NLL 3.700910\n",
      "epoch 5/10 | train NLL 3.725033\n",
      "epoch 5/10 | train NLL 3.696364\n",
      "epoch 5/10 | train NLL 3.676032\n",
      "epoch 5/10 | train NLL 3.674790\n",
      "epoch 5/10 | train NLL 3.655468\n",
      "epoch 5/10 | train NLL 3.611833\n",
      "epoch 5/10 | train NLL 3.630826\n",
      "epoch 5/10 | train NLL 4.440622\n",
      "epoch 5/10 | train NLL 4.669270\n",
      "epoch 5/10 | train NLL 5.142008\n",
      "epoch 5/10 | train NLL 5.828528\n",
      "epoch 5/10 | train NLL 6.304701\n",
      "epoch 5/10 | train NLL 7.046417\n",
      "epoch 5/10 | train NLL 7.455125\n",
      "epoch 5/10 | train NLL 7.791712\n",
      "epoch 5/10 | train NLL 8.075573\n",
      "epoch 5/10 | train NLL 8.543730\n",
      "epoch 5/10 | train NLL 8.687850\n",
      "epoch 5/10 | train NLL 9.449302\n",
      "epoch 5/10 | train NLL 9.506391\n",
      "epoch 5/10 | train NLL 9.448725\n",
      "epoch 5/10 | train NLL 9.693019\n",
      "epoch 5/10 | train NLL 9.692212\n",
      "epoch 5/10 | train NLL 9.594781\n",
      "epoch 5/10 | train NLL 9.637647\n",
      "epoch 5/10 | train NLL 9.642227\n",
      "epoch 5/10 | train NLL 9.577448\n",
      "epoch 5/10 | train NLL 9.463798\n",
      "epoch 5/10 | train NLL 9.403166\n",
      "epoch 5/10 | train NLL 9.300355\n",
      "epoch 5/10 | train NLL 9.220637\n",
      "epoch 5/10 | train NLL 9.109953\n",
      "epoch 5/10 | train NLL 8.999858\n",
      "epoch 5/10 | train NLL 8.925782\n",
      "epoch 5/10 | train NLL 8.814248\n",
      "epoch 5/10 | train NLL 8.723307\n",
      "epoch 5/10 | train NLL 8.670528\n",
      "epoch 5/10 | train NLL 8.618621\n",
      "epoch 5/10 | train NLL 8.554026\n",
      "epoch 5/10 | train NLL 8.475068\n",
      "epoch 5/10 | train NLL 8.453859\n",
      "epoch 5/10 | train NLL 8.410265\n",
      "epoch 5/10 | train NLL 8.332017\n",
      "epoch 5/10 | train NLL 8.253707\n",
      "epoch 5/10 | train NLL 8.199173\n",
      "epoch 5/10 | train NLL 8.174093\n",
      "epoch 5/10 | train NLL 8.130899\n",
      "epoch 5/10 | train NLL 8.084113\n",
      "epoch 5/10 | train NLL 7.997333\n",
      "epoch 5/10 | train NLL 7.938191\n",
      "epoch 5/10 | train NLL 7.933209\n",
      "epoch 5/10 | train NLL 7.904764\n",
      "epoch 5/10 | train NLL 7.820807\n",
      "epoch 5/10 | train NLL 7.768540\n",
      "epoch 5/10 | train NLL 7.717235\n",
      "epoch 5/10 | train NLL 7.661558\n",
      "epoch 6/10 | train NLL 4.180693\n",
      "epoch 6/10 | train NLL 3.951069\n",
      "epoch 6/10 | train NLL 4.052216\n",
      "epoch 6/10 | train NLL 4.428455\n",
      "epoch 6/10 | train NLL 4.216032\n",
      "epoch 6/10 | train NLL 4.169309\n",
      "epoch 6/10 | train NLL 4.152218\n",
      "epoch 6/10 | train NLL 4.137762\n",
      "epoch 6/10 | train NLL 4.218465\n",
      "epoch 6/10 | train NLL 4.145525\n",
      "epoch 6/10 | train NLL 4.033230\n",
      "epoch 6/10 | train NLL 4.050221\n",
      "epoch 6/10 | train NLL 4.042432\n",
      "epoch 6/10 | train NLL 3.964505\n",
      "epoch 6/10 | train NLL 3.975543\n",
      "epoch 6/10 | train NLL 3.968667\n",
      "epoch 6/10 | train NLL 3.863720\n",
      "epoch 6/10 | train NLL 3.856849\n",
      "epoch 6/10 | train NLL 3.822872\n",
      "epoch 6/10 | train NLL 3.792375\n",
      "epoch 6/10 | train NLL 3.804915\n",
      "epoch 6/10 | train NLL 3.834038\n",
      "epoch 6/10 | train NLL 3.839128\n",
      "epoch 6/10 | train NLL 3.819580\n",
      "epoch 6/10 | train NLL 3.792088\n",
      "epoch 6/10 | train NLL 3.754744\n",
      "epoch 6/10 | train NLL 3.740750\n",
      "epoch 6/10 | train NLL 3.742055\n",
      "epoch 6/10 | train NLL 3.707618\n",
      "epoch 6/10 | train NLL 3.714113\n",
      "epoch 6/10 | train NLL 3.728211\n",
      "epoch 6/10 | train NLL 3.718261\n",
      "epoch 6/10 | train NLL 3.701624\n",
      "epoch 6/10 | train NLL 3.682598\n",
      "epoch 6/10 | train NLL 3.690722\n",
      "epoch 6/10 | train NLL 3.678823\n",
      "epoch 6/10 | train NLL 3.643535\n",
      "epoch 6/10 | train NLL 3.597026\n",
      "epoch 6/10 | train NLL 3.573665\n",
      "epoch 6/10 | train NLL 3.553383\n",
      "epoch 6/10 | train NLL 3.540128\n",
      "epoch 6/10 | train NLL 3.516494\n",
      "epoch 6/10 | train NLL 3.509843\n",
      "epoch 6/10 | train NLL 3.495183\n",
      "epoch 6/10 | train NLL 3.478901\n",
      "epoch 6/10 | train NLL 3.475633\n",
      "epoch 6/10 | train NLL 3.456358\n",
      "epoch 6/10 | train NLL 3.459143\n",
      "epoch 6/10 | train NLL 3.435068\n",
      "epoch 6/10 | train NLL 3.408222\n",
      "epoch 6/10 | train NLL 3.386130\n",
      "epoch 6/10 | train NLL 3.370589\n",
      "epoch 6/10 | train NLL 3.367470\n",
      "epoch 6/10 | train NLL 3.353465\n",
      "epoch 6/10 | train NLL 3.343323\n",
      "epoch 6/10 | train NLL 3.332151\n",
      "epoch 6/10 | train NLL 3.326689\n",
      "epoch 6/10 | train NLL 3.313615\n",
      "epoch 6/10 | train NLL 3.301655\n",
      "epoch 6/10 | train NLL 3.282578\n",
      "epoch 6/10 | train NLL 3.262100\n",
      "epoch 6/10 | train NLL 3.264585\n",
      "epoch 6/10 | train NLL 3.247641\n",
      "epoch 6/10 | train NLL 3.242686\n",
      "epoch 6/10 | train NLL 3.242940\n",
      "epoch 6/10 | train NLL 3.239862\n",
      "epoch 6/10 | train NLL 3.235137\n",
      "epoch 7/10 | train NLL 2.737514\n",
      "epoch 7/10 | train NLL 2.595150\n",
      "epoch 7/10 | train NLL 2.547840\n",
      "epoch 7/10 | train NLL 2.692203\n",
      "epoch 7/10 | train NLL 2.604998\n",
      "epoch 7/10 | train NLL 2.800463\n",
      "epoch 7/10 | train NLL 2.750862\n",
      "epoch 7/10 | train NLL 2.710952\n",
      "epoch 7/10 | train NLL 2.683934\n",
      "epoch 7/10 | train NLL 2.612112\n",
      "epoch 7/10 | train NLL 2.510361\n",
      "epoch 7/10 | train NLL 2.483822\n",
      "epoch 7/10 | train NLL 2.498632\n",
      "epoch 7/10 | train NLL 2.545590\n",
      "epoch 7/10 | train NLL 2.576574\n",
      "epoch 7/10 | train NLL 2.553876\n",
      "epoch 7/10 | train NLL 2.495715\n",
      "epoch 7/10 | train NLL 2.510917\n",
      "epoch 7/10 | train NLL 2.517276\n",
      "epoch 7/10 | train NLL 2.506025\n",
      "epoch 7/10 | train NLL 2.513615\n",
      "epoch 7/10 | train NLL 2.500680\n",
      "epoch 7/10 | train NLL 2.524409\n",
      "epoch 7/10 | train NLL 2.491356\n",
      "epoch 7/10 | train NLL 2.455916\n",
      "epoch 7/10 | train NLL 2.467108\n",
      "epoch 7/10 | train NLL 2.478421\n",
      "epoch 7/10 | train NLL 2.483711\n",
      "epoch 7/10 | train NLL 2.465809\n",
      "epoch 7/10 | train NLL 2.442569\n",
      "epoch 7/10 | train NLL 2.421927\n",
      "epoch 7/10 | train NLL 2.402730\n",
      "epoch 7/10 | train NLL 2.405436\n",
      "epoch 7/10 | train NLL 2.400484\n",
      "epoch 7/10 | train NLL 2.382625\n",
      "epoch 7/10 | train NLL 2.389642\n",
      "epoch 7/10 | train NLL 2.378481\n",
      "epoch 7/10 | train NLL 2.375279\n",
      "epoch 7/10 | train NLL 2.361950\n",
      "epoch 7/10 | train NLL 2.354261\n",
      "epoch 7/10 | train NLL 2.356803\n",
      "epoch 7/10 | train NLL 2.354771\n",
      "epoch 7/10 | train NLL 2.409909\n",
      "epoch 7/10 | train NLL 2.403178\n",
      "epoch 7/10 | train NLL 2.383044\n",
      "epoch 7/10 | train NLL 2.385975\n",
      "epoch 7/10 | train NLL 2.376757\n",
      "epoch 7/10 | train NLL 2.372323\n",
      "epoch 7/10 | train NLL 2.366225\n",
      "epoch 7/10 | train NLL 2.349298\n",
      "epoch 7/10 | train NLL 2.349779\n",
      "epoch 7/10 | train NLL 2.346622\n",
      "epoch 7/10 | train NLL 2.335940\n",
      "epoch 7/10 | train NLL 2.332586\n",
      "epoch 7/10 | train NLL 2.327461\n",
      "epoch 7/10 | train NLL 2.328444\n",
      "epoch 7/10 | train NLL 2.316981\n",
      "epoch 7/10 | train NLL 2.306324\n",
      "epoch 7/10 | train NLL 2.302281\n",
      "epoch 7/10 | train NLL 2.289317\n",
      "epoch 7/10 | train NLL 2.283421\n",
      "epoch 7/10 | train NLL 2.275659\n",
      "epoch 7/10 | train NLL 2.358500\n",
      "epoch 7/10 | train NLL 2.345297\n",
      "epoch 7/10 | train NLL 2.344101\n",
      "epoch 7/10 | train NLL 2.437992\n",
      "epoch 7/10 | train NLL 2.432506\n",
      "epoch 8/10 | train NLL 2.403675\n",
      "epoch 8/10 | train NLL 1.937954\n",
      "epoch 8/10 | train NLL 1.906207\n",
      "epoch 8/10 | train NLL 1.935683\n",
      "epoch 8/10 | train NLL 1.909671\n",
      "epoch 8/10 | train NLL 1.908880\n",
      "epoch 8/10 | train NLL 2.695501\n",
      "epoch 8/10 | train NLL 2.583412\n",
      "epoch 8/10 | train NLL 2.508238\n",
      "epoch 8/10 | train NLL 2.983092\n",
      "epoch 8/10 | train NLL 2.859280\n",
      "epoch 8/10 | train NLL 2.789253\n",
      "epoch 8/10 | train NLL 2.691628\n",
      "epoch 8/10 | train NLL 2.615594\n",
      "epoch 8/10 | train NLL 2.575433\n",
      "epoch 8/10 | train NLL 2.506720\n",
      "epoch 8/10 | train NLL 2.447064\n",
      "epoch 8/10 | train NLL 2.425858\n",
      "epoch 8/10 | train NLL 2.393118\n",
      "epoch 8/10 | train NLL 2.340074\n",
      "epoch 8/10 | train NLL 2.319262\n",
      "epoch 8/10 | train NLL 2.481104\n",
      "epoch 8/10 | train NLL 2.430561\n",
      "epoch 8/10 | train NLL 2.697924\n",
      "epoch 8/10 | train NLL 2.669812\n",
      "epoch 8/10 | train NLL 2.718963\n",
      "epoch 8/10 | train NLL 2.696905\n",
      "epoch 8/10 | train NLL 2.807636\n",
      "epoch 8/10 | train NLL 2.845820\n",
      "epoch 8/10 | train NLL 2.900498\n",
      "epoch 8/10 | train NLL 2.867631\n",
      "epoch 8/10 | train NLL 2.969978\n",
      "epoch 8/10 | train NLL 2.998086\n",
      "epoch 8/10 | train NLL 2.958042\n",
      "epoch 8/10 | train NLL 2.917886\n",
      "epoch 8/10 | train NLL 2.992864\n",
      "epoch 8/10 | train NLL 3.024312\n",
      "epoch 8/10 | train NLL 2.991357\n",
      "epoch 8/10 | train NLL 3.038970\n",
      "epoch 8/10 | train NLL 2.992562\n",
      "epoch 8/10 | train NLL 2.952091\n",
      "epoch 8/10 | train NLL 2.922846\n",
      "epoch 8/10 | train NLL 2.911543\n",
      "epoch 8/10 | train NLL 2.913785\n",
      "epoch 8/10 | train NLL 3.116898\n",
      "epoch 8/10 | train NLL 3.167702\n",
      "epoch 8/10 | train NLL 3.167426\n",
      "epoch 8/10 | train NLL 3.156543\n",
      "epoch 8/10 | train NLL 3.138171\n",
      "epoch 8/10 | train NLL 3.111500\n",
      "epoch 8/10 | train NLL 3.083873\n",
      "epoch 8/10 | train NLL 3.055103\n",
      "epoch 8/10 | train NLL 3.043363\n",
      "epoch 8/10 | train NLL 3.028503\n",
      "epoch 8/10 | train NLL 3.005099\n",
      "epoch 8/10 | train NLL 2.980187\n",
      "epoch 8/10 | train NLL 2.951349\n",
      "epoch 8/10 | train NLL 3.017173\n",
      "epoch 8/10 | train NLL 2.999134\n",
      "epoch 8/10 | train NLL 3.184011\n",
      "epoch 8/10 | train NLL 4.018554\n",
      "epoch 8/10 | train NLL 4.195245\n",
      "epoch 8/10 | train NLL 4.593917\n",
      "epoch 8/10 | train NLL 5.179732\n",
      "epoch 8/10 | train NLL 5.953934\n",
      "epoch 8/10 | train NLL 6.481543\n",
      "epoch 8/10 | train NLL 6.878675\n",
      "epoch 9/10 | train NLL 47.917183\n",
      "epoch 9/10 | train NLL 39.551505\n",
      "epoch 9/10 | train NLL 42.192963\n",
      "epoch 9/10 | train NLL 43.684541\n",
      "epoch 9/10 | train NLL 41.062418\n",
      "epoch 9/10 | train NLL 42.979901\n",
      "epoch 9/10 | train NLL 42.234840\n",
      "epoch 9/10 | train NLL 42.279790\n",
      "epoch 9/10 | train NLL 42.391781\n",
      "epoch 9/10 | train NLL 41.843715\n",
      "epoch 9/10 | train NLL 41.264535\n",
      "epoch 9/10 | train NLL 41.531670\n",
      "epoch 9/10 | train NLL 41.272837\n",
      "epoch 9/10 | train NLL 40.790010\n",
      "epoch 9/10 | train NLL 39.899443\n",
      "epoch 9/10 | train NLL 39.267248\n",
      "epoch 9/10 | train NLL 39.039480\n",
      "epoch 9/10 | train NLL 38.368646\n",
      "epoch 9/10 | train NLL 38.119775\n",
      "epoch 9/10 | train NLL 38.029437\n",
      "epoch 9/10 | train NLL 37.866878\n",
      "epoch 9/10 | train NLL 37.892664\n",
      "epoch 9/10 | train NLL 37.379960\n",
      "epoch 9/10 | train NLL 37.304881\n",
      "epoch 9/10 | train NLL 36.856879\n",
      "epoch 9/10 | train NLL 36.816363\n",
      "epoch 9/10 | train NLL 36.656034\n",
      "epoch 9/10 | train NLL 36.622491\n",
      "epoch 9/10 | train NLL 36.192959\n",
      "epoch 9/10 | train NLL 35.771935\n",
      "epoch 9/10 | train NLL 35.550209\n",
      "epoch 9/10 | train NLL 35.436851\n",
      "epoch 9/10 | train NLL 35.268941\n",
      "epoch 9/10 | train NLL 35.256394\n",
      "epoch 9/10 | train NLL 35.058227\n",
      "epoch 9/10 | train NLL 34.567680\n",
      "epoch 9/10 | train NLL 34.363534\n",
      "epoch 9/10 | train NLL 34.233506\n",
      "epoch 9/10 | train NLL 34.155393\n",
      "epoch 9/10 | train NLL 34.061120\n",
      "epoch 9/10 | train NLL 33.784362\n",
      "epoch 9/10 | train NLL 33.385666\n",
      "epoch 9/10 | train NLL 33.194693\n",
      "epoch 9/10 | train NLL 33.198729\n",
      "epoch 9/10 | train NLL 33.029122\n",
      "epoch 9/10 | train NLL 32.936640\n",
      "epoch 9/10 | train NLL 32.750519\n",
      "epoch 9/10 | train NLL 32.663579\n",
      "epoch 9/10 | train NLL 32.504505\n",
      "epoch 9/10 | train NLL 32.305867\n",
      "epoch 9/10 | train NLL 32.198827\n",
      "epoch 9/10 | train NLL 32.040854\n",
      "epoch 9/10 | train NLL 31.936862\n",
      "epoch 9/10 | train NLL 31.854749\n",
      "epoch 9/10 | train NLL 31.722071\n",
      "epoch 9/10 | train NLL 31.649990\n",
      "epoch 9/10 | train NLL 31.432258\n",
      "epoch 9/10 | train NLL 31.238343\n",
      "epoch 9/10 | train NLL 31.050364\n",
      "epoch 9/10 | train NLL 31.024616\n",
      "epoch 9/10 | train NLL 30.901859\n",
      "epoch 9/10 | train NLL 30.743982\n",
      "epoch 9/10 | train NLL 30.518915\n",
      "epoch 9/10 | train NLL 30.306654\n",
      "epoch 9/10 | train NLL 30.166268\n",
      "epoch 9/10 | train NLL 29.925191\n",
      "epoch 9/10 | train NLL 29.824719\n",
      "epoch 10/10 | train NLL 18.978638\n",
      "epoch 10/10 | train NLL 18.431952\n",
      "epoch 10/10 | train NLL 21.142264\n",
      "epoch 10/10 | train NLL 21.717474\n",
      "epoch 10/10 | train NLL 21.983642\n",
      "epoch 10/10 | train NLL 21.559853\n",
      "epoch 10/10 | train NLL 21.517327\n",
      "epoch 10/10 | train NLL 21.468877\n",
      "epoch 10/10 | train NLL 21.566697\n",
      "epoch 10/10 | train NLL 21.380503\n",
      "epoch 10/10 | train NLL 21.486715\n",
      "epoch 10/10 | train NLL 21.162698\n",
      "epoch 10/10 | train NLL 20.836914\n",
      "epoch 10/10 | train NLL 20.490366\n",
      "epoch 10/10 | train NLL 20.570777\n",
      "epoch 10/10 | train NLL 20.158145\n",
      "epoch 10/10 | train NLL 20.005530\n",
      "epoch 10/10 | train NLL 20.140167\n",
      "epoch 10/10 | train NLL 20.169690\n",
      "epoch 10/10 | train NLL 20.029657\n",
      "epoch 10/10 | train NLL 19.831910\n",
      "epoch 10/10 | train NLL 19.686425\n",
      "epoch 10/10 | train NLL 19.542833\n",
      "epoch 10/10 | train NLL 19.361641\n",
      "epoch 10/10 | train NLL 19.443990\n",
      "epoch 10/10 | train NLL 19.563626\n",
      "epoch 10/10 | train NLL 19.758762\n",
      "epoch 10/10 | train NLL 19.776010\n",
      "epoch 10/10 | train NLL 19.709336\n",
      "epoch 10/10 | train NLL 19.616324\n",
      "epoch 10/10 | train NLL 19.587992\n",
      "epoch 10/10 | train NLL 19.833405\n",
      "epoch 10/10 | train NLL 19.771799\n",
      "epoch 10/10 | train NLL 19.738342\n",
      "epoch 10/10 | train NLL 19.801508\n",
      "epoch 10/10 | train NLL 19.652720\n",
      "epoch 10/10 | train NLL 19.809169\n",
      "epoch 10/10 | train NLL 19.658377\n",
      "epoch 10/10 | train NLL 19.521580\n",
      "epoch 10/10 | train NLL 19.386219\n",
      "epoch 10/10 | train NLL 19.498235\n",
      "epoch 10/10 | train NLL 19.364872\n",
      "epoch 10/10 | train NLL 19.328433\n",
      "epoch 10/10 | train NLL 19.373391\n",
      "epoch 10/10 | train NLL 19.368496\n",
      "epoch 10/10 | train NLL 19.213929\n",
      "epoch 10/10 | train NLL 19.127742\n",
      "epoch 10/10 | train NLL 19.066536\n",
      "epoch 10/10 | train NLL 19.058580\n",
      "epoch 10/10 | train NLL 19.140129\n",
      "epoch 10/10 | train NLL 19.089807\n",
      "epoch 10/10 | train NLL 18.994245\n",
      "epoch 10/10 | train NLL 18.960704\n",
      "epoch 10/10 | train NLL 19.026761\n",
      "epoch 10/10 | train NLL 18.902715\n",
      "epoch 10/10 | train NLL 18.910429\n",
      "epoch 10/10 | train NLL 18.769717\n",
      "epoch 10/10 | train NLL 18.647404\n",
      "epoch 10/10 | train NLL 18.600299\n",
      "epoch 10/10 | train NLL 18.576260\n",
      "epoch 10/10 | train NLL 18.449223\n",
      "epoch 10/10 | train NLL 18.368517\n",
      "epoch 10/10 | train NLL 18.315977\n",
      "epoch 10/10 | train NLL 18.280551\n",
      "epoch 10/10 | train NLL 18.209301\n",
      "epoch 10/10 | train NLL 18.155958\n",
      "epoch 10/10 | train NLL 18.204706\n",
      "TEST Acc: 0.125\n",
      "\n",
      "TEST metrics: {'MSE': 0.10079896450042725, 'RMSE': 0.31748852133750916, 'MAE': 0.13322485983371735, 'Cov@1σ': 0.7229999899864197, 'NLL': 15.406896591186523, 'Acc': 0.125}\n",
      "Final test metrics: {'MSE': 0.10079896450042725, 'RMSE': 0.31748852133750916, 'MAE': 0.13322485983371735, 'Cov@1σ': 0.7229999899864197, 'NLL': 15.406896591186523, 'Acc': 0.125}\n"
     ]
    }
   ],
   "source": [
    "# Example architecture\n",
    "# NOTE: pick sizes you like; this is a reasonable start.\n",
    "layer_sizes = [784, 10, 5, 10]\n",
    "K = 5\n",
    "\n",
    "bnn = BNN_GMM(layer_sizes=layer_sizes, K=K, bias=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "bnn.to(device)\n",
    "\n",
    "metrics = run_mnist_classification_with_your_trainer(\n",
    "    bnn=bnn,\n",
    "    device=device,\n",
    "    epochs=10,\n",
    "    batch_size=12,\n",
    "    lr=1e-4,\n",
    "    input_sigma=1e-2,\n",
    "    last_relu=False,     # keep last layer linear (recommended)\n",
    "    normalize=True,\n",
    "    max_components=50,\n",
    ")\n",
    "\n",
    "print(\"Final test metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba571704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn.layers[1].pi_w.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
